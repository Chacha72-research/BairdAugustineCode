{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to get historical unlevered cash flows :o\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "#straightforward functions, use it if needed\n",
    "def get_financial_data(ticker):\n",
    "    stock = yf.Ticker(ticker)\n",
    "    financials = stock.financials\n",
    "    return financials\n",
    "\n",
    "def get_cash_flow_data(ticker):\n",
    "    stock = yf.Ticker(ticker)\n",
    "    cash_flow = stock.cashflow\n",
    "    return cash_flow\n",
    "\n",
    "def get_income_statement_data(ticker):\n",
    "    stock = yf.Ticker(ticker)\n",
    "    income_statement = stock.financials\n",
    "    return income_statement\n",
    "\n",
    "def get_balance_sheet_data(ticker):\n",
    "    stock = yf.Ticker(ticker)\n",
    "    balance_sheet = stock.balance_sheet\n",
    "    return balance_sheet\n",
    "#get EBIT \n",
    "def get_ebit(ticker):\n",
    "    try:\n",
    "        stock = yf.Ticker(ticker)\n",
    "        financials = stock.financials.T #transpose to get it in that tabular format with dates and values on right column\n",
    "        if 'EBIT' not in financials.columns:\n",
    "            raise ValueError(\"EBIT data is not available.\")\n",
    "        return financials['EBIT']\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting EBIT: {e}\")\n",
    "        return pd.Series() #holds sequence of EBIT values\n",
    "\n",
    "def get_tax_rate(ticker):\n",
    "    try:\n",
    "        stock = yf.Ticker(ticker)\n",
    "        financials = stock.financials.T #same transpose\n",
    "        tax_rates = {}\n",
    "        for date in financials.index:\n",
    "            try:\n",
    "                tax_expense = financials.loc[date, 'Tax Provision']\n",
    "                pretax_income = financials.loc[date, 'Pretax Income']\n",
    "                if pretax_income != 0:\n",
    "                    tax_rate = tax_expense / pretax_income\n",
    "                    tax_rates[date] = tax_rate\n",
    "                else:\n",
    "                    tax_rates[date] = None\n",
    "            except KeyError as e:\n",
    "                print(f\"KeyError: {e} for date {date}\")\n",
    "                tax_rates[date] = None\n",
    "            except ZeroDivisionError:\n",
    "                tax_rates[date] = None\n",
    "        tax_rates_df = pd.DataFrame(list(tax_rates.items()), columns=['Date', 'Tax Rate']) #dataframe or table of historical tax rates\n",
    "        tax_rates_df.set_index('Date', inplace=True)\n",
    "        return tax_rates_df['Tax Rate']\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting tax rate: {e}\")\n",
    "        return pd.Series()\n",
    "\n",
    "def get_nopat(ticker):\n",
    "    try:\n",
    "        ebit_series = get_ebit(ticker)\n",
    "        tax_rate_series = get_tax_rate(ticker)\n",
    "        if ebit_series.empty or tax_rate_series.empty:\n",
    "            raise ValueError(\"Unable to calculate NOPAT due to missing EBIT or tax rate data.\")\n",
    "        ebit_series = ebit_series.loc[tax_rate_series.index]\n",
    "        tax_rate_series = tax_rate_series.loc[ebit_series.index]\n",
    "        nopat_series = ebit_series * (1 - tax_rate_series) #match indexes (dates) and calculate based on matching index in the datafram table format\n",
    "        return nopat_series\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating NOPAT: {e}\")\n",
    "        return pd.Series()\n",
    "\n",
    "def get_depreciation_amortization(ticker):\n",
    "    try:\n",
    "        stock = yf.Ticker(ticker)\n",
    "        cashflow = stock.cashflow.T\n",
    "        if 'Depreciation Amortization Depletion' not in cashflow.columns:\n",
    "            raise ValueError(\"D&A data is not available.\")\n",
    "        return cashflow['Depreciation Amortization Depletion']\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting D&A: {e}\")\n",
    "        return pd.Series()\n",
    "\n",
    "def get_capex(ticker):\n",
    "    try:\n",
    "        stock = yf.Ticker(ticker)\n",
    "        cashflow = stock.cashflow.T\n",
    "        if 'Capital Expenditure' not in cashflow.columns:\n",
    "            raise ValueError(\"CapEx data is not available.\")\n",
    "        return cashflow['Capital Expenditure']\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting CapEx: {e}\")\n",
    "        return pd.Series()\n",
    "\n",
    "def get_nwc(ticker):\n",
    "    try:\n",
    "        stock = yf.Ticker(ticker)\n",
    "        balance_sheet = stock.balance_sheet.T\n",
    "        current_assets = balance_sheet.get('Current Assets', pd.Series())\n",
    "        current_liabilities = balance_sheet.get('Current Liabilities', pd.Series())\n",
    "        if not current_assets.empty and not current_liabilities.empty:\n",
    "            nwc = current_assets - current_liabilities\n",
    "        else:\n",
    "            raise ValueError(\"Current Assets or Current Liabilities data is missing.\")\n",
    "        return nwc\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting NWC: {e}\")\n",
    "        return pd.Series()\n",
    "\n",
    "def calculate_nwc_increase(ticker):\n",
    "    try:\n",
    "        nwc_series = get_nwc(ticker)\n",
    "        if nwc_series.empty:\n",
    "            raise ValueError(\"NWC data is not available.\")\n",
    "        nwc_change = nwc_series.diff()\n",
    "        return nwc_change\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating NWC increase: {e}\")\n",
    "        return pd.Series()\n",
    "\n",
    "def calculate_ufcf(ticker):\n",
    "    try:\n",
    "        nopat_series = get_nopat(ticker)\n",
    "        da_series = get_depreciation_amortization(ticker)\n",
    "        capex_series = get_capex(ticker)\n",
    "        nwc_increase_series = calculate_nwc_increase(ticker)\n",
    "        if nopat_series.empty or da_series.empty or capex_series.empty or nwc_increase_series.empty:\n",
    "            raise ValueError(\"One or more required data series are missing.\")\n",
    "        common_index = nopat_series.index.intersection(da_series.index).intersection(capex_series.index).intersection(nwc_increase_series.index) #merge series values based on common index which are the dates\n",
    "        #align based on that common index which is date\n",
    "        nopat_series = nopat_series.loc[common_index]\n",
    "        da_series = da_series.loc[common_index]\n",
    "        capex_series = capex_series.loc[common_index]\n",
    "        nwc_increase_series = nwc_increase_series.loc[common_index]\n",
    "        # UFCF = NOPAT + Depreciation & Amortization - Increase in NWC - Capital Expenditures\n",
    "        ufcs = nopat_series + da_series - nwc_increase_series - capex_series #after merging based on index, you can actually calculate\n",
    "        return ufcs\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating UFCF: {e}\")\n",
    "        return pd.Series()\n",
    "\n",
    "def process_tickers():\n",
    "    results = []\n",
    "    tickers = pd.read_csv('AllDatav2.csv')['Ticker']\n",
    "    try:\n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                ebitda = get_ebit(ticker)\n",
    "                depamo = get_depreciation_amortization(ticker)\n",
    "                capex = get_capex(ticker)\n",
    "                increase_in_nwc = calculate_nwc_increase(ticker)\n",
    "                ufcs = calculate_ufcf(ticker)\n",
    "                result = {\n",
    "                    'Ticker': ticker,\n",
    "                    'Ebitda': ebitda,\n",
    "                    'Depreciation/Amortization': depamo,\n",
    "                    'Capital Expenditures': capex,\n",
    "                    'Net Working Capital Increase Per Year': increase_in_nwc,\n",
    "                    'UFCF': ufcs\n",
    "                }\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {ticker}: {e}\")\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.to_csv('DCFalgorithm.csv', index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing tickers: {e}\")\n",
    "\n",
    "process_tickers()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "def get_historical_tax_rates(ticker):\n",
    "    # Fetch financial data\n",
    "    stock = yf.Ticker(ticker)\n",
    "    financials = stock.financials.T  \n",
    "\n",
    "    # Initialize a dictionary to hold tax rates for each period\n",
    "    tax_rates = {}\n",
    "\n",
    "    # Calculate tax rate for each period\n",
    "    for date in financials.index:\n",
    "        try:\n",
    "            tax_expense = financials.loc[date, 'Tax Provision']\n",
    "            pretax_income = financials.loc[date, 'Pretax Income']\n",
    "            \n",
    "            # Avoid division by zero\n",
    "            if pretax_income != 0:\n",
    "                tax_rate = tax_expense / pretax_income\n",
    "                tax_rates[date] = tax_rate\n",
    "            else:\n",
    "                tax_rates[date] = None\n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError: {e} for date {date}\")\n",
    "            tax_rates[date] = None\n",
    "        except ZeroDivisionError:\n",
    "            tax_rates[date] = None\n",
    "\n",
    "    # Convert the dictionary to a DataFrame for easier analysis\n",
    "    tax_rates_df = pd.DataFrame(list(tax_rates.items()), columns=['Date', 'Tax Rate'])\n",
    "    tax_rates_df.set_index('Date', inplace=True)\n",
    "    \n",
    "    return tax_rates_df\n",
    "\n",
    "# Example usage\n",
    "ticker = 'AAPL'\n",
    "tax_rates_df = get_historical_tax_rates(ticker)\n",
    "print(tax_rates_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('DCFalgorithm.csv', index_col='Ticker')\n",
    "\n",
    "print(df.head())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file clean up\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extract_numbers(cell):\n",
    "    # Remove the \"Name:\" and \"dtype:\" parts\n",
    "    cell = re.sub(r'Name:.*?dtype:.*', '', str(cell))\n",
    "    \n",
    "    # Find all numbers (including negative numbers and NaN), excluding date components\n",
    "    numbers = re.findall(r'-?\\d+(?:\\.\\d+)?(?:e[-+]?\\d+)?|\\bNaN\\b', cell)\n",
    "    \n",
    "    # Filter out numbers that are likely to be years, months, or days\n",
    "    numbers = [num for num in numbers if not (abs(float(num.replace('NaN', '0'))) <= 31 or 1900 <= abs(float(num.replace('NaN', '0'))) <= 2100)]\n",
    "    \n",
    "    # Replace 'NaN' with '0'\n",
    "    numbers = ['0' if num.lower() == 'nan' else num for num in numbers]\n",
    "    \n",
    "    return numbers\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('DCFalgorithm.csv')\n",
    "\n",
    "# Process each column\n",
    "for column in df.columns:\n",
    "    # Extract numbers from the column\n",
    "    extracted = df[column].apply(extract_numbers)\n",
    "    \n",
    "    # Find the maximum number of values in any row\n",
    "    max_values = extracted.apply(len).max()\n",
    "    \n",
    "    # Create new columns for each value\n",
    "    for i in range(max_values):\n",
    "        df[f'{column}_{i+1}'] = extracted.apply(lambda x: x[i] if i < len(x) else None)\n",
    "    \n",
    "    # Drop the original column\n",
    "    df = df.drop(columns=[column])\n",
    "\n",
    "# Display the results\n",
    "print(df)\n",
    "\n",
    "# If you want to save the results to a new CSV file\n",
    "df.to_csv('processed_data5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "file_path = 'processed_data6.csv'  \n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Columns to reshape\n",
    "metrics = [\n",
    "    'Ebitda', 'Depreciation/Amortization', 'Capital Expenditures', \n",
    "    'Net Working Capital Increase Per Year', 'UFCF'\n",
    "]\n",
    "\n",
    "# Reshape data to long format\n",
    "data_long = pd.wide_to_long(data, stubnames=metrics, i='Ticker', j='Year', sep='_', suffix='\\d+').reset_index()\n",
    "\n",
    "# Sorting by Ticker and Year\n",
    "data_long = data_long.sort_values(by=['Ticker', 'Year'])\n",
    "\n",
    "# Display the transformed data\n",
    "print(data_long.head())\n",
    "\n",
    "output_path = 'processed_data_ml.csv'  \n",
    "data_long.to_csv(output_path, index=False)\n",
    "\n",
    "import os\n",
    "if os.path.exists(output_path):\n",
    "    print(f'Transformed data has been saved to {output_path}')\n",
    "else:\n",
    "    print('Error in saving the transformed data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neural network trained to predict Unlevered Cash Flows using financial statement data and valuation/stock metrics!!!! Extremeley low error!!!\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "# Load valuation metrics and processed data\n",
    "valuation_metrics_path = 'valuation_metrics_official.csv'\n",
    "processed_data_path = 'processed_data_ml.csv'\n",
    "additional_data_path = 'AllDatav2.csv'\n",
    "\n",
    "valuation_metrics_df = pd.read_csv(valuation_metrics_path)\n",
    "processed_data_df = pd.read_csv(processed_data_path)\n",
    "additional_data_df = pd.read_csv(additional_data_path)\n",
    "\n",
    "# Convert integer Year values to actual years\n",
    "year_mapping = {1: 2021, 2: 2022, 3: 2023, 4: 2024}\n",
    "processed_data_df['Year'] = processed_data_df['Year'].map(year_mapping)\n",
    "\n",
    "# Sort by Ticker and Year to ensure temporal order\n",
    "processed_data_df = processed_data_df.sort_values(by=['Ticker', 'Year'])\n",
    "\n",
    "# Label encoding for Ticker\n",
    "le = LabelEncoder()\n",
    "processed_data_df['Ticker'] = le.fit_transform(processed_data_df['Ticker'])\n",
    "valuation_metrics_df['Ticker'] = le.transform(valuation_metrics_df['Ticker'])\n",
    "additional_data_df['Ticker'] = le.transform(additional_data_df['Ticker'])\n",
    "\n",
    "# Merge the additional metrics\n",
    "data_df = pd.merge(processed_data_df, valuation_metrics_df, on='Ticker', how='left')\n",
    "data_df = pd.merge(data_df, additional_data_df, on='Ticker', how='left')\n",
    "\n",
    "# Create lag features and rolling means\n",
    "data_df['Lag_Ebitda'] = data_df.groupby('Ticker')['Ebitda'].shift(1)\n",
    "data_df['Lag_Depreciation'] = data_df.groupby('Ticker')['Depreciation/Amortization'].shift(1)\n",
    "data_df['Lag_CapEx'] = data_df.groupby('Ticker')['Capital Expenditures'].shift(1)\n",
    "data_df['Lag_NWC'] = data_df.groupby('Ticker')['Net Working Capital Increase Per Year'].shift(1)\n",
    "\n",
    "data_df['Rolling_Mean_Ebitda'] = data_df.groupby('Ticker')['Ebitda'].rolling(window=2).mean().reset_index(0, drop=True)\n",
    "data_df['Rolling_Mean_Depreciation'] = data_df.groupby('Ticker')['Depreciation/Amortization'].rolling(window=2).mean().reset_index(0, drop=True)\n",
    "data_df['Rolling_Mean_CapEx'] = data_df.groupby('Ticker')['Capital Expenditures'].rolling(window=2).mean().reset_index(0, drop=True)\n",
    "data_df['Rolling_Mean_NWC'] = data_df.groupby('Ticker')['Net Working Capital Increase Per Year'].rolling(window=2).mean().reset_index(0, drop=True)\n",
    "\n",
    "# Drop rows with NaN values resulting from shifting\n",
    "data_df = data_df.dropna()\n",
    "\n",
    "# Define features and target\n",
    "X = data_df.drop(columns=['UFCF'])\n",
    "y = data_df['UFCF']\n",
    "\n",
    "# Handle missing values\n",
    "X = X.fillna(0)\n",
    "y = y.fillna(0)\n",
    "\n",
    "# Fetch and process financial statements for additional features\n",
    "def fetch_financial_data(ticker):\n",
    "    stock = yf.Ticker(ticker)\n",
    "    financials = stock.financials\n",
    "    balance_sheet = stock.balance_sheet\n",
    "    cashflow = stock.cashflow\n",
    "    \n",
    "    data = {\n",
    "        'Revenue': financials.loc['Total Revenue'].values[0] if 'Total Revenue' in financials.index else 0,\n",
    "        'COGS': financials.loc['Cost Of Revenue'].values[0] if 'Cost Of Revenue' in financials.index else 0,\n",
    "        'Operating Expenses': financials.loc['Operating Expense'].values[0] if 'Operating Expense' in financials.index else 0,\n",
    "        'Depreciation': cashflow.loc['Depreciation And Amortization'].values[0] if 'Depreciation And Amortization' in cashflow.index else 0,\n",
    "        'CapEx': cashflow.loc['Capital Expenditure'].values[0] if 'Capital Expenditure' in cashflow.index else 0,\n",
    "        'Gross Profit': financials.loc['Gross Profit'].values[0] if 'Gross Profit' in financials.index else 0,\n",
    "        'EBIT': financials.loc['EBIT'].values[0] if 'EBIT' in financials.index else 0,\n",
    "        'EBITDA': financials.loc['EBITDA'].values[0] if 'EBITDA' in financials.index else 0,\n",
    "        'Total Assets': balance_sheet.loc['Total Assets'].values[0] if 'Total Assets' in balance_sheet.index else 0,\n",
    "        'Total Liabilities': balance_sheet.loc['Total Liabilities Net Minority Interest'].values[0] if 'Total Liabilities Net Minority Interest' in balance_sheet.index else 0,\n",
    "        'Total Equity': balance_sheet.loc['Total Equity Gross Minority Interest'].values[0] if 'Total Equity Gross Minority Interest' in balance_sheet.index else 0,\n",
    "        'Net Income': financials.loc['Net Income'].values[0] if 'Net Income' in financials.index else 0,\n",
    "        'Operating Cash Flow': cashflow.loc['Cash Flow From Continuing Operating Activities'].values[0] if 'Cash Flow From Continuing Operating Activities' in cashflow.index else 0,\n",
    "        'Free Cash Flow': cashflow.loc['Free Cash Flow'].values[0] if 'Free Cash Flow' in cashflow.index else 0\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame([data])\n",
    "\n",
    "financial_features_list = [fetch_financial_data(ticker) for ticker in le.classes_]\n",
    "\n",
    "# Concatenate all data\n",
    "financial_features_df = pd.concat(financial_features_list, ignore_index=True)\n",
    "\n",
    "# Integrate financial data with main dataset\n",
    "data_df = pd.concat([data_df.reset_index(drop=True), financial_features_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Define features and target again with additional financial data\n",
    "X = data_df.drop(columns=['UFCF'])\n",
    "y = data_df['UFCF']\n",
    "\n",
    "# Handle any potential missing values\n",
    "X = X.fillna(0)\n",
    "y = y.fillna(0)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "joblib.dump(scaler, 'scaler2.pkl') #save scaler\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test).flatten()\n",
    "\n",
    "# Ensure there are no NaN values in predictions\n",
    "if np.any(np.isnan(y_pred)):\n",
    "    raise ValueError(\"Predictions contain NaN values.\")\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = np.mean(np.abs(y_test - y_pred))\n",
    "\n",
    "print(f\"Test MSE: {mse}\")\n",
    "print(f\"Test RMSE: {rmse}\")\n",
    "print(f\"Test MAE: {mae}\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Custom scorer for permutation importance\n",
    "def custom_scorer(estimator, X, y):\n",
    "    y_pred = estimator.predict(X).flatten()\n",
    "    return -mean_squared_error(y, y_pred)\n",
    "\n",
    "# Compute permutation feature importance\n",
    "result = permutation_importance(model, X_test, y_test, scoring=custom_scorer, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Feature importance plot\n",
    "# Create a dataframe for feature importances\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': result.importances_mean\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(feature_importances_df['Feature'], feature_importances_df['Importance'], color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importances')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# Identify features with negative importance scores\n",
    "negative_importance_features = feature_importances_df[feature_importances_df['Importance'] < 0]['Feature'].tolist()\n",
    "\n",
    "# Drop these features from the dataset\n",
    "X_filtered = data_df.drop(columns=['UFCF'] + negative_importance_features)\n",
    "y_filtered = data_df['UFCF']\n",
    "\n",
    "# Handle any potential missing values\n",
    "X_filtered = X_filtered.fillna(0)\n",
    "y_filtered = y_filtered.fillna(0)\n",
    "\n",
    "# Scale features\n",
    "X_filtered_scaled = scaler.fit_transform(X_filtered)\n",
    "\n",
    "# Split the data\n",
    "X_train_filtered, X_test_filtered, y_train_filtered, y_test_filtered = train_test_split(X_filtered_scaled, y_filtered, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the neural network model again for filtered data\n",
    "model_filtered = Sequential()\n",
    "model_filtered.add(Dense(64, activation='relu', input_shape=(X_train_filtered.shape[1],)))\n",
    "model_filtered.add(Dropout(0.3))\n",
    "model_filtered.add(Dense(32, activation='relu'))\n",
    "model_filtered.add(Dropout(0.3))\n",
    "model_filtered.add(Dense(16, activation='relu'))\n",
    "model_filtered.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model_filtered.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history_filtered = model_filtered.fit(X_train_filtered, y_train_filtered, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_filtered = model_filtered.predict(X_test_filtered).flatten()\n",
    "\n",
    "# Ensure there are no NaN values in predictions\n",
    "if np.any(np.isnan(y_pred_filtered)):\n",
    "    raise ValueError(\"Predictions contain NaN values.\")\n",
    "\n",
    "mse_filtered = mean_squared_error(y_test_filtered, y_pred_filtered)\n",
    "rmse_filtered = np.sqrt(mse_filtered)\n",
    "mae_filtered = np.mean(np.abs(y_test_filtered - y_pred_filtered))\n",
    "\n",
    "print(f\"Filtered Test MSE: {mse_filtered}\")\n",
    "print(f\"Filtered Test RMSE: {rmse_filtered}\")\n",
    "print(f\"Filtered Test MAE: {mae_filtered}\")\n",
    "\n",
    "# Plot training history for filtered model\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history_filtered.history['loss'], label='Filtered Training Loss')\n",
    "plt.plot(history_filtered.history['val_loss'], label='Filtered Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss for Filtered Model')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the entire model\n",
    "model.save('UFCFmodelrevised.h5') \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing with random data!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_historical_data(ticker, start_date, end_date):\n",
    "    stock = yf.Ticker(ticker)\n",
    "    \n",
    "    # Fetch quarterly financial data\n",
    "    income_stmt = stock.quarterly_financials\n",
    "    balance_sheet = stock.quarterly_balance_sheet\n",
    "    cash_flow = stock.quarterly_cashflow\n",
    "    \n",
    "    # Extract relevant metrics with error handling\n",
    "    def safe_get(df, keys):\n",
    "        for key in keys:\n",
    "            if key in df.index:\n",
    "                return df.loc[key]\n",
    "        return pd.Series(np.nan, index=df.columns)\n",
    "\n",
    "    revenue = safe_get(income_stmt, ['Total Revenue', 'Revenue'])\n",
    "    ebitda = safe_get(income_stmt, ['EBITDA', 'EBIT'])\n",
    "    capex = safe_get(cash_flow, ['Capital Expenditure', 'Capital Expenditures', 'Property Plant Equipment'])\n",
    "    \n",
    "    # Calculate Net Working Capital (Current Assets - Current Liabilities)\n",
    "    current_assets = safe_get(balance_sheet, ['Total Current Assets', 'Total Assets'])\n",
    "    current_liabilities = safe_get(balance_sheet, ['Total Current Liabilities', 'Total Liabilities Net Minority Interest'])\n",
    "    nwc = current_assets - current_liabilities\n",
    "    \n",
    "    # Combine all metrics into a single DataFrame\n",
    "    data = pd.concat([revenue, ebitda, capex, nwc], axis=1)\n",
    "    data.columns = ['Revenue', 'EBITDA', 'CapEx', 'NWC']\n",
    "    \n",
    "    # Calculate UFCF (Unlevered Free Cash Flow)\n",
    "    data['UFCF'] = data['EBITDA'] - data['CapEx'] - data['NWC'].diff()\n",
    "    \n",
    "    return data\n",
    "\n",
    "# List of tickers you want to analyze\n",
    "tickers = pd.read_csv('AllDatav2.csv')['Ticker'].tolist()  # Convert to list\n",
    "\n",
    "# Fetch data for each ticker and combine\n",
    "all_data = []\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        data = get_historical_data(ticker, '2010-01-01', '2023-12-31')\n",
    "        data['Ticker'] = ticker\n",
    "        all_data.append(data)\n",
    "        print(f\"Successfully processed {ticker}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {ticker}: {str(e)}\")\n",
    "\n",
    "# Combine all data into a single DataFrame\n",
    "combined_data = pd.concat(all_data, axis=0)\n",
    "\n",
    "# Reset index to turn dates into a column\n",
    "combined_data = combined_data.reset_index()\n",
    "combined_data.columns = ['Date'] + list(combined_data.columns[1:])\n",
    "\n",
    "# Save to CSV\n",
    "combined_data.to_csv('historical_data.csv', index=False)\n",
    "print(\"Data saved to 'historical_data.csv'\")\n",
    "\n",
    "#remmebe that it is historical data so 20 quarters =  5years haha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in historical_data csv, make sure to use backfill/frontfill to NaN values (preserves trend for LSTM model but unfortunately won't be perfect accuracy when using in ML model)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('historical_data.csv', parse_dates=['Date'])\n",
    "\n",
    "# Set Date as index\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "# List of columns to fill\n",
    "columns_to_fill = ['Revenue', 'EBITDA', 'CapEx', 'NWC', 'UFCF']\n",
    "\n",
    "# Apply backward fill to each column\n",
    "for col in columns_to_fill:\n",
    "    df[col] = df[col].bfill()\n",
    "\n",
    "# Reset index to make Date a column again\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "print(df)\n",
    "\n",
    "df.to_csv('historical_data_updated.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date     Revenue      EBITDA      CapEx          NWC        UFCF  \\\n",
      "14 2022-11-30  4816000000  1896000000 -121000000  14838000000  1141000000   \n",
      "15 2023-02-28  4816000000  1896000000 -121000000  14838000000  1141000000   \n",
      "16 2023-05-31  4816000000  1896000000 -121000000  14838000000  1141000000   \n",
      "17 2023-08-31  4890000000  1988000000  -91000000  15776000000  1141000000   \n",
      "18 2023-11-30  5048000000  2058000000  -47000000  16518000000  1363000000   \n",
      "\n",
      "   Ticker  \n",
      "14   ADBE  \n",
      "15   ADBE  \n",
      "16   ADBE  \n",
      "17   ADBE  \n",
      "18   ADBE  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('historical_data_final.csv')\n",
    "\n",
    "# Convert the Date column to datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Sort the data by Ticker and Date\n",
    "df = df.sort_values(by=['Ticker', 'Date'])\n",
    "\n",
    "# Save the sorted data to a new CSV file\n",
    "df.to_csv('historical_data_final2.csv', index=False)\n",
    "\n",
    "# Display the first few rows to verify sorting\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough data points for ticker ADBE to create sequences.\n",
      "Skipping ticker ADBE due to insufficient data.\n",
      "Not enough data points for ticker ORCL to create sequences.\n",
      "Skipping ticker ORCL due to insufficient data.\n",
      "Not enough data points for ticker KARO to create sequences.\n",
      "Skipping ticker KARO due to insufficient data.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 81\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ticker \u001b[38;5;129;01min\u001b[39;00m tickers:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         train_scaled, test_scaled, scaler, train, test \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mticker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_train_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;66;03m# Determine sequence length based on available data\u001b[39;00m\n\u001b[1;32m     84\u001b[0m         sequence_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n_train_samples, \u001b[38;5;28mlen\u001b[39m(train_scaled) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 52\u001b[0m, in \u001b[0;36mprepare_data\u001b[0;34m(df, ticker, n_train)\u001b[0m\n\u001b[1;32m     49\u001b[0m df_ticker \u001b[38;5;241m=\u001b[39m df_ticker[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRevenue\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Fetch financial data\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m financial_data \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_financial_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mticker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Merge revenue data with financial data\u001b[39;00m\n\u001b[1;32m     55\u001b[0m df_combined \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(df_ticker, financial_data, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdropna()\n",
      "Cell \u001b[0;32mIn[9], line 30\u001b[0m, in \u001b[0;36mfetch_financial_data\u001b[0;34m(ticker)\u001b[0m\n\u001b[1;32m     27\u001b[0m income_statement[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(income_statement[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Fetch balance sheet\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m balance_sheet \u001b[38;5;241m=\u001b[39m \u001b[43mstock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbalance_sheet\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m     31\u001b[0m balance_sheet \u001b[38;5;241m=\u001b[39m balance_sheet\u001b[38;5;241m.\u001b[39mreset_index()\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m     32\u001b[0m balance_sheet[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(balance_sheet[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/yfinance/ticker.py:212\u001b[0m, in \u001b[0;36mTicker.balance_sheet\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbalance_sheet\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_balance_sheet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/yfinance/base.py:351\u001b[0m, in \u001b[0;36mTickerBase.get_balance_sheet\u001b[0;34m(self, proxy, as_dict, pretty, freq)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;124;03m:Parameters:\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    as_dict: bool\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;124;03m        Default is None\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fundamentals\u001b[38;5;241m.\u001b[39mproxy \u001b[38;5;241m=\u001b[39m proxy \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproxy\n\u001b[0;32m--> 351\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fundamentals\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinancials\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_balance_sheet_time_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfreq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pretty:\n\u001b[1;32m    354\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/yfinance/scrapers/fundamentals.py:61\u001b[0m, in \u001b[0;36mFinancials.get_balance_sheet_time_series\u001b[0;34m(self, freq, proxy)\u001b[0m\n\u001b[1;32m     59\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_balance_sheet_time_series\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m freq \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m res:\n\u001b[0;32m---> 61\u001b[0m     res[freq] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch_time_series\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbalance-sheet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res[freq]\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/yfinance/utils.py:104\u001b[0m, in \u001b[0;36mlog_indent_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEntering \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m IndentationContext():\n\u001b[0;32m--> 104\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExiting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/yfinance/scrapers/fundamentals.py:85\u001b[0m, in \u001b[0;36mFinancials._fetch_time_series\u001b[0;34m(self, name, timescale, proxy)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIllegal argument: timescale must be one of: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mallowed_timescales\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     statement \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_financials_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimescale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m statement \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m statement\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/yfinance/scrapers/fundamentals.py:101\u001b[0m, in \u001b[0;36mFinancials._create_financials_table\u001b[0;34m(self, name, timescale, proxy)\u001b[0m\n\u001b[1;32m     98\u001b[0m keys \u001b[38;5;241m=\u001b[39m const\u001b[38;5;241m.\u001b[39mfundamentals_keys[name]\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_financials_time_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimescale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/yfinance/scrapers/fundamentals.py:141\u001b[0m, in \u001b[0;36mFinancials.get_financials_time_series\u001b[0;34m(self, timescale, keys, proxy)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m         df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39mdates, index\u001b[38;5;241m=\u001b[39m[k])\n\u001b[0;32m--> 141\u001b[0m     df\u001b[38;5;241m.\u001b[39mloc[k] \u001b[38;5;241m=\u001b[39m {pd\u001b[38;5;241m.\u001b[39mTimestamp(x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masOfDate\u001b[39m\u001b[38;5;124m\"\u001b[39m]): x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreportedValue\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m v}\n\u001b[1;32m    143\u001b[0m df\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m timescale, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Reorder table to match order on Yahoo website\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/pandas/core/indexing.py:911\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[1;32m    910\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[0;32m--> 911\u001b[0m \u001b[43miloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/pandas/core/indexing.py:1944\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1942\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_with_indexer_split_path(indexer, value, name)\n\u001b[1;32m   1943\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1944\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_single_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/pandas/core/indexing.py:2189\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_single_block\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   2183\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Series\n\u001b[1;32m   2185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(value, ABCSeries) \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m   2186\u001b[0m     \u001b[38;5;66;03m# TODO(EA): ExtensionBlock.setitem this causes issues with\u001b[39;00m\n\u001b[1;32m   2187\u001b[0m     \u001b[38;5;66;03m# setting for extensionarrays that store dicts. Need to decide\u001b[39;00m\n\u001b[1;32m   2188\u001b[0m     \u001b[38;5;66;03m# if it's worth supporting that.\u001b[39;00m\n\u001b[0;32m-> 2189\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_align_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2191\u001b[0m info_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_info_axis_number\n\u001b[1;32m   2192\u001b[0m item_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(info_axis)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/pandas/core/indexing.py:2453\u001b[0m, in \u001b[0;36m_iLocIndexer._align_series\u001b[0;34m(self, indexer, ser, multiindex_indexer, using_cow)\u001b[0m\n\u001b[1;32m   2450\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ser\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mequals(ax):\n\u001b[1;32m   2451\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ser\u001b[38;5;241m.\u001b[39m_values\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m-> 2453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   2455\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible indexer with Series\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/pandas/core/series.py:5153\u001b[0m, in \u001b[0;36mSeries.reindex\u001b[0;34m(self, index, axis, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[1;32m   5136\u001b[0m \u001b[38;5;129m@doc\u001b[39m(\n\u001b[1;32m   5137\u001b[0m     NDFrame\u001b[38;5;241m.\u001b[39mreindex,  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m   5138\u001b[0m     klass\u001b[38;5;241m=\u001b[39m_shared_doc_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mklass\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5151\u001b[0m     tolerance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5152\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[0;32m-> 5153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/pandas/core/generic.py:5610\u001b[0m, in \u001b[0;36mNDFrame.reindex\u001b[0;34m(self, labels, index, columns, axis, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[1;32m   5607\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_multi(axes, copy, fill_value)\n\u001b[1;32m   5609\u001b[0m \u001b[38;5;66;03m# perform the reindex on the axes\u001b[39;00m\n\u001b[0;32m-> 5610\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reindex_axes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5611\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\n\u001b[1;32m   5612\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreindex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/pandas/core/generic.py:5633\u001b[0m, in \u001b[0;36mNDFrame._reindex_axes\u001b[0;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[1;32m   5630\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   5632\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(a)\n\u001b[0;32m-> 5633\u001b[0m new_index, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtolerance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\n\u001b[1;32m   5635\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5637\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(a)\n\u001b[1;32m   5638\u001b[0m obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[1;32m   5639\u001b[0m     {axis: [new_index, indexer]},\n\u001b[1;32m   5640\u001b[0m     fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[1;32m   5641\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m   5642\u001b[0m     allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   5643\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/pandas/core/indexes/base.py:4418\u001b[0m, in \u001b[0;36mIndex.reindex\u001b[0;34m(self, target, method, level, limit, tolerance)\u001b[0m\n\u001b[1;32m   4413\u001b[0m     target, indexer, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_join_level(\n\u001b[1;32m   4414\u001b[0m         target, level, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m, keep_order\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_multi\n\u001b[1;32m   4415\u001b[0m     )\n\u001b[1;32m   4417\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4418\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mequals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   4419\u001b[0m         indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4420\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/pandas/core/indexes/datetimelike.py:175\u001b[0m, in \u001b[0;36mDatetimeIndexOpsMixin.equals\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m other\u001b[38;5;241m.\u001b[39mdtype:\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# have different timezone\u001b[39;00m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masi8, \u001b[43mother\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masi8\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/pandas/core/indexes/datetimelike.py:108\u001b[0m, in \u001b[0;36mDatetimeIndexOpsMixin.asi8\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;129m@freq\u001b[39m\u001b[38;5;241m.\u001b[39msetter\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfreq\u001b[39m(\u001b[38;5;28mself\u001b[39m, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# error: Property \"freq\" defined in \"PeriodArray\" is read-only  [misc]\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mfreq \u001b[38;5;241m=\u001b[39m value  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21masi8\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mint64]:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39masi8\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@doc\u001b[39m(DatetimeLikeArrayMixin\u001b[38;5;241m.\u001b[39mfreqstr)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfreqstr\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "# Load the historical revenue data\n",
    "df = pd.read_csv('historical_data_final2.csv')\n",
    "\n",
    "# Convert the Date column to datetime and sort by date\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.sort_values(by='Date')\n",
    "\n",
    "# Extract unique tickers\n",
    "tickers = df['Ticker'].unique()\n",
    "\n",
    "# Function to fetch and prepare financial data using yfinance\n",
    "def fetch_financial_data(ticker):\n",
    "    stock = yf.Ticker(ticker)\n",
    "    \n",
    "    # Fetch income statement\n",
    "    income_statement = stock.financials.T\n",
    "    income_statement = income_statement.reset_index().rename(columns={'index': 'Date'})\n",
    "    income_statement['Date'] = pd.to_datetime(income_statement['Date'])\n",
    "    \n",
    "    # Fetch balance sheet\n",
    "    balance_sheet = stock.balance_sheet.T\n",
    "    balance_sheet = balance_sheet.reset_index().rename(columns={'index': 'Date'})\n",
    "    balance_sheet['Date'] = pd.to_datetime(balance_sheet['Date'])\n",
    "    \n",
    "    # Fetch cash flow statement\n",
    "    cash_flow = stock.cashflow.T\n",
    "    cash_flow = cash_flow.reset_index().rename(columns={'index': 'Date'})\n",
    "    cash_flow['Date'] = pd.to_datetime(cash_flow['Date'])\n",
    "\n",
    "    # Combine into a single DataFrame\n",
    "    financial_data = pd.merge(income_statement, balance_sheet, on='Date', how='outer')\n",
    "    financial_data = pd.merge(financial_data, cash_flow, on='Date', how='outer')\n",
    "    \n",
    "    return financial_data\n",
    "\n",
    "# Prepare data with additional financial features\n",
    "def prepare_data(df, ticker, n_train):\n",
    "    # Filter historical revenue data for the specific ticker\n",
    "    df_ticker = df[df['Ticker'] == ticker]\n",
    "    df_ticker = df_ticker[['Date', 'Revenue']]\n",
    "\n",
    "    # Fetch financial data\n",
    "    financial_data = fetch_financial_data(ticker)\n",
    "    \n",
    "    # Merge revenue data with financial data\n",
    "    df_combined = pd.merge(df_ticker, financial_data, on='Date', how='left').dropna()\n",
    "\n",
    "    # Ensure there are enough data points\n",
    "    if len(df_combined) < 2 * n_train:\n",
    "        raise ValueError(f\"Not enough data points for ticker {ticker} to create sequences.\")\n",
    "\n",
    "    # Split the data based on the number of samples\n",
    "    train = df_combined[:n_train]\n",
    "    test = df_combined[n_train:n_train*2]\n",
    "\n",
    "    # Normalize the features\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_scaled = scaler.fit_transform(train.drop(columns=['Date']))\n",
    "    test_scaled = scaler.transform(test.drop(columns=['Date']))\n",
    "\n",
    "    return train_scaled, test_scaled, scaler, train, test\n",
    "\n",
    "# Define the number of training samples\n",
    "n_train_samples = 3\n",
    "\n",
    "# Create models for each ticker\n",
    "models = {}\n",
    "results = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        train_scaled, test_scaled, scaler, train, test = prepare_data(df, ticker, n_train_samples)\n",
    "        \n",
    "        # Determine sequence length based on available data\n",
    "        sequence_length = min(n_train_samples, len(train_scaled) - 1)\n",
    "        \n",
    "        # Create sequences for LSTM\n",
    "        generator = TimeseriesGenerator(train_scaled, train_scaled[:, 0], length=sequence_length, batch_size=1)\n",
    "        \n",
    "        # Build the model\n",
    "        model = Sequential([\n",
    "            LSTM(50, activation='relu', input_shape=(sequence_length, train_scaled.shape[1])),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(generator, epochs=10, verbose=1)\n",
    "        \n",
    "        # Save the model\n",
    "        models[ticker] = model\n",
    "\n",
    "        # Predict the next 5 quarters\n",
    "        last_sequence = np.array([train_scaled[-sequence_length:]])\n",
    "        predictions_scaled = []\n",
    "        \n",
    "        for _ in range(5):\n",
    "            # Make prediction\n",
    "            prediction = model.predict(last_sequence)[0][0]\n",
    "            predictions_scaled.append(prediction)\n",
    "            \n",
    "            # Update last_sequence with new prediction\n",
    "            new_sequence = np.concatenate((last_sequence[:, 1:, :], [[prediction]]), axis=1)\n",
    "\n",
    "        # Inverse transform predictions\n",
    "        predictions = scaler.inverse_transform(np.array(predictions_scaled).reshape(-1, 1))\n",
    "        \n",
    "        # Evaluate the model\n",
    "        mae = mse = rmse = None\n",
    "        if len(test) > sequence_length:\n",
    "            test_sequences = TimeseriesGenerator(test_scaled, test_scaled[:, 0], length=sequence_length, batch_size=1)\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            \n",
    "            for i in range(len(test_sequences)):\n",
    "                x, y = test_sequences[i]\n",
    "                y_true.append(y[0])\n",
    "                y_pred.append(model.predict(x)[0][0])\n",
    "            \n",
    "            y_true = np.array(y_true)\n",
    "            y_pred = np.array(y_pred)\n",
    "            \n",
    "            mae = mean_absolute_error(y_true, y_pred)\n",
    "            mse = mean_squared_error(y_true, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            \n",
    "            print(f\"Metrics for {ticker}:\")\n",
    "            print(f\"  MAE: {mae}\")\n",
    "            print(f\"  MSE: {mse}\")\n",
    "            print(f\"  RMSE: {rmse}\")\n",
    "        else:\n",
    "            print(f\"Not enough test data for evaluation for ticker {ticker}\")\n",
    "        \n",
    "        print(f\"Predictions for {ticker}: {predictions.flatten()}\")\n",
    "\n",
    "        # Store results\n",
    "        for i, pred in enumerate(predictions.flatten(), start=1):\n",
    "            results.append([ticker, f'Q{i}', pred, mae, mse, rmse])\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        print(f\"Skipping ticker {ticker} due to insufficient data.\")\n",
    "\n",
    "# Create DataFrame from results\n",
    "results_df = pd.DataFrame(results, columns=['Ticker', 'Quarter', 'Prediction', 'MAE', 'MSE', 'RMSE'])\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv('RevenuePredictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 71\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         all_data\u001b[38;5;241m.\u001b[39mappend(data)\n\u001b[0;32m---> 71\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Combine all data into one DataFrame\u001b[39;00m\n\u001b[1;32m     74\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(all_data)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#LSTM model to predict revenues based on past financial statements\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "aa = pd.read_csv('AllDatav2.csv')\n",
    "\n",
    "# Define a list of tickers\n",
    "tickers = aa['Ticker'].unique().tolist()\n",
    "\n",
    "# Function to load revenue data using yfinance\n",
    "def load_financial_data(ticker):\n",
    "    try:\n",
    "        # Fetch the financial data\n",
    "        stock = yf.Ticker(ticker)\n",
    "        financials = stock.financials.transpose()  # Transpose to have dates as index\n",
    "        \n",
    "        # Extract the Revenue data\n",
    "        if 'Total Revenue' not in financials.columns:\n",
    "            raise ValueError(f\"Revenue data not available for ticker {ticker}\")\n",
    "        \n",
    "        revenue_data = financials[['Total Revenue']]\n",
    "        revenue_data = revenue_data.dropna().reset_index()\n",
    "        revenue_data.columns = ['Date', 'Revenue']\n",
    "        revenue_data['Date'] = pd.to_datetime(revenue_data['Date'])\n",
    "        revenue_data['Ticker'] = ticker\n",
    "        \n",
    "        return revenue_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Prepare data function (modified to handle yfinance data)\n",
    "def prepare_data(df, ticker, cutoff_date):\n",
    "    df_ticker = df[df['Ticker'] == ticker]\n",
    "    df_ticker = df_ticker[['Date', 'Revenue']]\n",
    "    \n",
    "    # Split the data\n",
    "    train = df_ticker[df_ticker['Date'] < cutoff_date]\n",
    "    test = df_ticker[df_ticker['Date'] >= cutoff_date]\n",
    "    \n",
    "    if len(train) < 3:\n",
    "        raise ValueError(f\"Not enough data points for ticker {ticker} to create sequences.\")\n",
    "    \n",
    "    # Normalize the revenue\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_scaled = scaler.fit_transform(train[['Revenue']])\n",
    "    test_scaled = scaler.transform(test[['Revenue']])\n",
    "    \n",
    "    return train_scaled, test_scaled, scaler, train, test\n",
    "\n",
    "# Define the cutoff date\n",
    "cutoff_date = '2022-06-30'\n",
    "\n",
    "# Create models for each ticker\n",
    "models = {}\n",
    "results = []\n",
    "all_data = []\n",
    "\n",
    "# Load data for each ticker\n",
    "for ticker in tickers:\n",
    "    data = load_financial_data(ticker)\n",
    "    if data is not None:\n",
    "        all_data.append(data)\n",
    "    time.sleep(2)\n",
    "\n",
    "# Combine all data into one DataFrame\n",
    "df = pd.concat(all_data)\n",
    "\n",
    "# Process data and train models\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        train_scaled, test_scaled, scaler, train, test = prepare_data(df, ticker, cutoff_date)\n",
    "        \n",
    "        # Determine sequence length based on available data\n",
    "        sequence_length = min(2, len(train_scaled) - 1)\n",
    "        \n",
    "        # Create sequences for LSTM\n",
    "        generator = TimeseriesGenerator(train_scaled, train_scaled, length=sequence_length, batch_size=1)\n",
    "        \n",
    "        # Build the model\n",
    "        model = Sequential([\n",
    "            LSTM(50, activation='relu', input_shape=(sequence_length, 1)),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(generator, epochs=10, verbose=1)\n",
    "        \n",
    "        # Save the model\n",
    "        models[ticker] = model\n",
    "\n",
    "        # Predict the next 5 quarters\n",
    "        last_sequence = np.array([train_scaled[-sequence_length:]])\n",
    "        predictions_scaled = []\n",
    "        \n",
    "        for _ in range(5):\n",
    "            # Make prediction\n",
    "            prediction = model.predict(last_sequence)[0][0]\n",
    "            predictions_scaled.append(prediction)\n",
    "            \n",
    "            # Update last_sequence with new prediction\n",
    "            new_sequence = np.array([[[prediction]]])\n",
    "            last_sequence = np.concatenate((last_sequence[:, 1:, :], new_sequence), axis=1)\n",
    "\n",
    "        # Inverse transform predictions\n",
    "        predictions = scaler.inverse_transform(np.array(predictions_scaled).reshape(-1, 1))\n",
    "        \n",
    "        # Evaluate the model\n",
    "        mae = mse = rmse = None\n",
    "        if len(test) > sequence_length:\n",
    "            test_sequences = TimeseriesGenerator(test_scaled, test_scaled, length=sequence_length, batch_size=1)\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            \n",
    "            for i in range(len(test_sequences)):\n",
    "                x, y = test_sequences[i]\n",
    "                y_true.append(y[0, 0])\n",
    "                y_pred.append(model.predict(x)[0, 0])\n",
    "            \n",
    "            y_true = np.array(y_true)\n",
    "            y_pred = np.array(y_pred)\n",
    "            \n",
    "            mae = mean_absolute_error(y_true, y_pred)\n",
    "            mse = mean_squared_error(y_true, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            \n",
    "            print(f\"Metrics for {ticker}:\")\n",
    "            print(f\"  MAE: {mae}\")\n",
    "            print(f\"  MSE: {mse}\")\n",
    "            print(f\"  RMSE: {rmse}\")\n",
    "        else:\n",
    "            print(f\"Not enough test data for evaluation for ticker {ticker}\")\n",
    "        \n",
    "        print(f\"Predictions for {ticker}: {predictions.flatten()}\")\n",
    "\n",
    "        # Store results\n",
    "        for i, pred in enumerate(predictions.flatten(), start=1):\n",
    "            results.append([ticker, f'Q{i}', pred, mae, mse, rmse])\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        print(f\"Skipping ticker {ticker} due to insufficient data.\")\n",
    "\n",
    "# Create DataFrame from results\n",
    "results_df = pd.DataFrame(results, columns=['Ticker', 'Quarter', 'Prediction', 'MAE', 'MSE', 'RMSE'])\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv('Revenue_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-09 13:39:58,308 - INFO - Successfully fetched data for CRM\n",
      "2024-08-09 13:39:58,373 - INFO - Successfully fetched data for ADBE\n",
      "2024-08-09 13:39:58,378 - INFO - Successfully fetched data for ORCL\n",
      "2024-08-09 13:39:58,392 - INFO - Successfully fetched data for INTU\n",
      "2024-08-09 13:39:58,411 - INFO - Successfully fetched data for NFLX\n",
      "2024-08-09 13:39:59,481 - INFO - Successfully fetched data for PANW\n",
      "2024-08-09 13:39:59,491 - INFO - Successfully fetched data for ADP\n",
      "2024-08-09 13:39:59,529 - INFO - Successfully fetched data for CRWD\n",
      "2024-08-09 13:39:59,557 - INFO - Successfully fetched data for NOW\n",
      "2024-08-09 13:39:59,716 - INFO - Successfully fetched data for SHOP\n",
      "2024-08-09 13:40:00,247 - INFO - Successfully fetched data for WDAY\n",
      "2024-08-09 13:40:00,515 - INFO - Successfully fetched data for SPOT\n",
      "2024-08-09 13:40:00,605 - INFO - Successfully fetched data for ADSK\n",
      "2024-08-09 13:40:00,651 - INFO - Successfully fetched data for PLTR\n",
      "2024-08-09 13:40:00,721 - INFO - Successfully fetched data for TTD\n",
      "2024-08-09 13:40:01,313 - INFO - Successfully fetched data for TEAM\n",
      "2024-08-09 13:40:01,417 - INFO - Successfully fetched data for DDOG\n",
      "2024-08-09 13:40:01,485 - INFO - Successfully fetched data for FTNT\n",
      "2024-08-09 13:40:01,703 - INFO - Successfully fetched data for SNOW\n",
      "2024-08-09 13:40:01,762 - INFO - Successfully fetched data for HUBS\n",
      "2024-08-09 13:40:02,290 - INFO - Successfully fetched data for APP\n",
      "2024-08-09 13:40:02,466 - INFO - Successfully fetched data for ZS\n",
      "2024-08-09 13:40:02,496 - INFO - Successfully fetched data for NET\n",
      "2024-08-09 13:40:02,524 - INFO - Successfully fetched data for ANSS\n",
      "2024-08-09 13:40:02,670 - INFO - Successfully fetched data for MSTR\n",
      "2024-08-09 13:40:03,078 - INFO - Successfully fetched data for ZM\n",
      "2024-08-09 13:40:03,352 - INFO - Successfully fetched data for MDB\n",
      "2024-08-09 13:40:03,468 - INFO - Successfully fetched data for GDDY\n",
      "2024-08-09 13:40:03,625 - INFO - Successfully fetched data for CHKP\n",
      "2024-08-09 13:40:03,644 - INFO - Successfully fetched data for IOT\n",
      "2024-08-09 13:40:04,240 - INFO - Successfully fetched data for SSNC\n",
      "2024-08-09 13:40:04,437 - INFO - Successfully fetched data for GEN\n",
      "2024-08-09 13:40:04,677 - INFO - Successfully fetched data for OKTA\n",
      "2024-08-09 13:40:04,793 - INFO - Successfully fetched data for TOST\n",
      "2024-08-09 13:40:04,984 - INFO - Successfully fetched data for BSY\n",
      "2024-08-09 13:40:05,257 - INFO - Successfully fetched data for MNDY\n",
      "2024-08-09 13:40:05,347 - INFO - Successfully fetched data for AKAM\n",
      "2024-08-09 13:40:05,596 - INFO - Successfully fetched data for DT\n",
      "2024-08-09 13:40:05,610 - INFO - Successfully fetched data for GWRE\n",
      "2024-08-09 13:40:05,895 - INFO - Successfully fetched data for CYBR\n",
      "2024-08-09 13:40:06,179 - INFO - Successfully fetched data for ESTC\n",
      "2024-08-09 13:40:06,432 - INFO - Successfully fetched data for NICE\n",
      "2024-08-09 13:40:06,471 - INFO - Successfully fetched data for DOCU\n",
      "2024-08-09 13:40:06,613 - INFO - Successfully fetched data for FFIV\n",
      "2024-08-09 13:40:06,618 - INFO - Successfully fetched data for TWLO\n",
      "2024-08-09 13:40:07,221 - INFO - Successfully fetched data for INFA\n",
      "2024-08-09 13:40:07,334 - INFO - Successfully fetched data for CFLT\n",
      "2024-08-09 13:40:07,338 - INFO - Successfully fetched data for WIX\n",
      "2024-08-09 13:40:07,521 - INFO - Successfully fetched data for APPF\n",
      "2024-08-09 13:40:07,652 - INFO - Successfully fetched data for PAYC\n",
      "2024-08-09 13:40:07,779 - INFO - Successfully fetched data for OTEX\n",
      "2024-08-09 13:40:07,944 - INFO - Successfully fetched data for DSGX\n",
      "2024-08-09 13:40:08,376 - INFO - Successfully fetched data for DBX\n",
      "2024-08-09 13:40:08,383 - INFO - Successfully fetched data for GTLB\n",
      "2024-08-09 13:40:08,388 - INFO - Successfully fetched data for PCTY\n",
      "2024-08-09 13:40:08,394 - INFO - Successfully fetched data for MTCH\n",
      "2024-08-09 13:40:08,637 - INFO - Successfully fetched data for PATH\n",
      "2024-08-09 13:40:08,724 - INFO - Successfully fetched data for HCP\n",
      "2024-08-09 13:40:08,833 - INFO - Successfully fetched data for SPSC\n",
      "2024-08-09 13:40:09,051 - INFO - Successfully fetched data for KVYO\n",
      "2024-08-09 13:40:09,247 - INFO - Successfully fetched data for SQSP\n",
      "2024-08-09 13:40:09,308 - INFO - Successfully fetched data for CCCS\n",
      "2024-08-09 13:40:09,340 - INFO - Successfully fetched data for SMAR\n",
      "2024-08-09 13:40:09,416 - INFO - Successfully fetched data for S\n",
      "2024-08-09 13:40:09,605 - INFO - Successfully fetched data for VERX\n",
      "2024-08-09 13:40:09,809 - INFO - Successfully fetched data for VRNS\n",
      "2024-08-09 13:40:09,866 - INFO - Successfully fetched data for BILL\n",
      "2024-08-09 13:40:10,067 - INFO - Successfully fetched data for QLYS\n",
      "2024-08-09 13:40:10,069 - INFO - Successfully fetched data for DOCS\n",
      "2024-08-09 13:40:10,255 - INFO - Successfully fetched data for PEGA\n",
      "2024-08-09 13:40:10,505 - INFO - Successfully fetched data for CWAN\n",
      "2024-08-09 13:40:10,745 - INFO - Successfully fetched data for TENB\n",
      "2024-08-09 13:40:10,814 - INFO - Successfully fetched data for PWSC\n",
      "2024-08-09 13:40:10,841 - INFO - Successfully fetched data for ALIT\n",
      "2024-08-09 13:40:10,869 - INFO - Successfully fetched data for ZI\n",
      "2024-08-09 13:40:11,045 - INFO - Successfully fetched data for FROG\n",
      "2024-08-09 13:40:11,199 - INFO - Successfully fetched data for BOX\n",
      "2024-08-09 13:40:11,427 - INFO - Successfully fetched data for BRZE\n",
      "2024-08-09 13:40:11,445 - INFO - Successfully fetched data for FRSH\n",
      "2024-08-09 13:40:11,450 - INFO - Successfully fetched data for ZETA\n",
      "2024-08-09 13:40:11,516 - INFO - Successfully fetched data for NCNO\n",
      "2024-08-09 13:40:11,685 - INFO - Successfully fetched data for AI\n",
      "2024-08-09 13:40:11,736 - INFO - Successfully fetched data for DV\n",
      "2024-08-09 13:40:11,871 - INFO - Successfully fetched data for INST\n",
      "2024-08-09 13:40:11,883 - INFO - Successfully fetched data for DOCN\n",
      "2024-08-09 13:40:11,948 - INFO - Successfully fetched data for FIVN\n",
      "2024-08-09 13:40:12,218 - INFO - Successfully fetched data for ASAN\n",
      "2024-08-09 13:40:12,357 - INFO - Successfully fetched data for LSPD\n",
      "2024-08-09 13:40:12,379 - INFO - Successfully fetched data for MQ\n",
      "2024-08-09 13:40:12,455 - INFO - Successfully fetched data for AGYS\n",
      "2024-08-09 13:40:12,637 - INFO - Successfully fetched data for NABL\n",
      "2024-08-09 13:40:12,887 - INFO - Successfully fetched data for ALKT\n",
      "2024-08-09 13:40:12,905 - INFO - Successfully fetched data for RPD\n",
      "2024-08-09 13:40:12,977 - INFO - Successfully fetched data for CERT\n",
      "2024-08-09 13:40:13,069 - INFO - Successfully fetched data for CXM\n",
      "2024-08-09 13:40:13,084 - INFO - Successfully fetched data for RNG\n",
      "2024-08-09 13:40:13,212 - INFO - Successfully fetched data for PYCR\n",
      "2024-08-09 13:40:13,284 - INFO - Successfully fetched data for APPN\n",
      "2024-08-09 13:40:13,335 - INFO - Successfully fetched data for PD\n",
      "2024-08-09 13:40:13,438 - INFO - Successfully fetched data for SWI\n",
      "2024-08-09 13:40:13,485 - INFO - Successfully fetched data for JAMF\n",
      "2024-08-09 13:40:13,557 - INFO - Successfully fetched data for EVCM\n",
      "2024-08-09 13:40:13,679 - INFO - Successfully fetched data for RAMP\n",
      "2024-08-09 13:40:13,756 - INFO - Successfully fetched data for SPT\n",
      "2024-08-09 13:40:14,204 - INFO - Successfully fetched data for SEMR\n",
      "2024-08-09 13:40:14,205 - INFO - Successfully fetched data for AVPT\n",
      "2024-08-09 13:40:14,234 - INFO - Successfully fetched data for DFIN\n",
      "2024-08-09 13:40:14,293 - INFO - Successfully fetched data for TDOC\n",
      "2024-08-09 13:40:14,394 - INFO - Successfully fetched data for DCBO\n",
      "2024-08-09 13:40:14,585 - INFO - Successfully fetched data for IAS\n",
      "2024-08-09 13:40:14,703 - INFO - Successfully fetched data for PAR\n",
      "2024-08-09 13:40:14,878 - INFO - Successfully fetched data for ZUO\n",
      "2024-08-09 13:40:14,926 - INFO - Successfully fetched data for LZ\n",
      "2024-08-09 13:40:15,048 - INFO - Successfully fetched data for RUM\n",
      "2024-08-09 13:40:15,226 - INFO - Successfully fetched data for ETWO\n",
      "2024-08-09 13:40:15,292 - INFO - Successfully fetched data for MTTR\n",
      "2024-08-09 13:40:15,375 - INFO - Successfully fetched data for PRO\n",
      "2024-08-09 13:40:15,453 - INFO - Successfully fetched data for BMBL\n",
      "2024-08-09 13:40:15,525 - INFO - Successfully fetched data for VTEX\n",
      "2024-08-09 13:40:15,574 - INFO - Successfully fetched data for WKME\n",
      "2024-08-09 13:40:15,735 - INFO - Successfully fetched data for SOUN\n",
      "2024-08-09 13:40:15,790 - INFO - Successfully fetched data for CSGS\n",
      "2024-08-09 13:40:15,899 - INFO - Successfully fetched data for UDMY\n",
      "2024-08-09 13:40:15,912 - INFO - Successfully fetched data for GENI\n",
      "2024-08-09 13:40:15,990 - INFO - Successfully fetched data for MODN\n",
      "2024-08-09 13:40:16,114 - INFO - Successfully fetched data for RSKD\n",
      "2024-08-09 13:40:16,222 - INFO - Successfully fetched data for COUR\n",
      "2024-08-09 13:40:16,306 - INFO - Successfully fetched data for AMPL\n",
      "2024-08-09 13:40:16,500 - INFO - Successfully fetched data for SABR\n",
      "2024-08-09 13:40:16,507 - INFO - Successfully fetched data for KARO\n",
      "2024-08-09 13:40:16,613 - INFO - Successfully fetched data for PUBM\n",
      "2024-08-09 13:40:16,814 - INFO - Successfully fetched data for FSLY\n",
      "2024-08-09 13:40:16,928 - INFO - Successfully fetched data for BASE\n",
      "2024-08-09 13:40:16,951 - INFO - Successfully fetched data for TTGT\n",
      "2024-08-09 13:40:17,086 - INFO - Successfully fetched data for OLO\n",
      "2024-08-09 13:40:17,187 - INFO - Successfully fetched data for KC\n",
      "2024-08-09 13:40:17,242 - INFO - Successfully fetched data for YEXT\n",
      "2024-08-09 13:40:17,278 - INFO - Successfully fetched data for RXT\n",
      "2024-08-09 13:40:17,345 - INFO - Successfully fetched data for THRY\n",
      "2024-08-09 13:40:17,434 - INFO - Successfully fetched data for WEAV\n",
      "2024-08-09 13:40:18,025 - INFO - Successfully fetched data for VMEO\n",
      "2024-08-09 13:40:18,071 - INFO - Successfully fetched data for BIGC\n",
      "2024-08-09 13:40:18,076 - INFO - Successfully fetched data for BLND\n",
      "2024-08-09 13:40:18,179 - INFO - Successfully fetched data for SMWB\n",
      "2024-08-09 13:40:18,207 - INFO - Successfully fetched data for CGNT\n",
      "2024-08-09 13:40:18,393 - INFO - Successfully fetched data for MITK\n",
      "2024-08-09 13:40:18,457 - INFO - Successfully fetched data for PERI\n",
      "2024-08-09 13:40:18,650 - INFO - Successfully fetched data for EB\n",
      "2024-08-09 13:40:18,696 - INFO - Successfully fetched data for OSPN\n",
      "2024-08-09 13:40:18,755 - INFO - Successfully fetched data for HCAT\n",
      "2024-08-09 13:40:18,839 - INFO - Successfully fetched data for LAW\n",
      "2024-08-09 13:40:19,037 - INFO - Successfully fetched data for DOMO\n",
      "2024-08-09 13:40:19,056 - INFO - Successfully fetched data for SPOK\n",
      "2024-08-09 13:40:19,175 - INFO - Successfully fetched data for NRDY\n",
      "2024-08-09 13:40:19,354 - INFO - Successfully fetched data for EGHT\n",
      "2024-08-09 13:40:19,439 - INFO - Successfully fetched data for OB\n",
      "2024-08-09 13:40:19,493 - INFO - Successfully fetched data for AMSWA\n",
      "2024-08-09 13:40:19,660 - INFO - Successfully fetched data for BLZE\n",
      "2024-08-09 13:40:19,732 - INFO - Successfully fetched data for ASUR\n",
      "2024-08-09 13:40:19,823 - INFO - Successfully fetched data for NOTE\n",
      "2024-08-09 13:40:19,926 - INFO - Successfully fetched data for KLTR\n",
      "2024-08-09 13:40:20,056 - INFO - Successfully fetched data for EGAN\n",
      "2024-08-09 13:40:20,153 - INFO - Successfully fetched data for APPS\n",
      "2024-08-09 13:40:20,236 - INFO - Successfully fetched data for EXFY\n",
      "2024-08-09 13:40:20,340 - INFO - Successfully fetched data for UPLD\n",
      "2024-08-09 13:40:20,587 - INFO - Successfully fetched data for IDN\n",
      "2024-08-09 13:40:20,624 - INFO - Successfully fetched data for LPSN\n",
      "2024-08-09 13:40:20,689 - INFO - Successfully fetched data for APCX\n",
      "2024-08-09 13:40:20,696 - INFO - Successfully fetched data for AWRE\n",
      "2024-08-09 13:40:20,873 - INFO - Successfully fetched data for XELA\n",
      "2024-08-09 13:40:20,994 - INFO - Successfully fetched data for BLBX\n",
      "2024-08-09 13:40:20,995 - INFO - Successfully fetched data for TWOU\n",
      "2024-08-09 13:40:21,171 - INFO - Successfully fetched data for MRIN\n",
      "/var/folders/mf/zm3qz5852hs67j4m4hwml0z80000gn/T/ipykernel_48978/1534565859.py:72: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  df = df.groupby('Ticker').apply(lambda group: group.set_index('Date').asfreq('M').ffill().bfill()).reset_index()\n",
      "/var/folders/mf/zm3qz5852hs67j4m4hwml0z80000gn/T/ipykernel_48978/1534565859.py:72: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.groupby('Ticker').apply(lambda group: group.set_index('Date').asfreq('M').ffill().bfill()).reset_index()\n",
      "/var/folders/mf/zm3qz5852hs67j4m4hwml0z80000gn/T/ipykernel_48978/1534565859.py:72: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('Ticker').apply(lambda group: group.set_index('Date').asfreq('M').ffill().bfill()).reset_index()\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot insert Ticker, already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mf/zm3qz5852hs67j4m4hwml0z80000gn/T/ipykernel_48978/1534565859.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mcommon_start_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Ticker'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mcommon_start_date\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;31m# Forward fill missing values and backfill if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Ticker'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masfreq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'M'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m# Prepare data function (modified to handle yfinance data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, level, drop, inplace, col_level, col_fill, allow_duplicates, names)\u001b[0m\n\u001b[1;32m   6468\u001b[0m                     level_values = algorithms.take(\n\u001b[1;32m   6469\u001b[0m                         \u001b[0mlevel_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_na_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6470\u001b[0m                     )\n\u001b[1;32m   6471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6472\u001b[0;31m                 new_obj.insert(\n\u001b[0m\u001b[1;32m   6473\u001b[0m                     \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6474\u001b[0m                     \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6475\u001b[0m                     \u001b[0mlevel_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myenv/lib/python3.10/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, loc, column, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   5154\u001b[0m                 \u001b[0;34m\"'self.flags.allows_duplicate_labels' is False.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5155\u001b[0m             )\n\u001b[1;32m   5156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_duplicates\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5157\u001b[0m             \u001b[0;31m# Should this be a different kind of error??\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5158\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"cannot insert {column}, already exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5159\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5160\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loc must be int\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5161\u001b[0m         \u001b[0;31m# convert non stdlib ints to satisfy typing checks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot insert Ticker, already exists"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import logging\n",
    "\n",
    "# Set up logging to track the progress\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "aa = pd.read_csv('AllDatav2.csv')\n",
    "\n",
    "# Define a list of tickers\n",
    "tickers = aa['Ticker'].unique().tolist()\n",
    "\n",
    "# Function to load revenue data using yfinance with retry logic\n",
    "def load_financial_data(ticker):\n",
    "    for attempt in range(3):  # Retry up to 3 times\n",
    "        try:\n",
    "            # Fetch the financial data\n",
    "            stock = yf.Ticker(ticker)\n",
    "            financials = stock.financials.transpose()  # Transpose to have dates as index\n",
    "            \n",
    "            # Extract the Revenue data\n",
    "            if 'Total Revenue' not in financials.columns:\n",
    "                raise ValueError(f\"Revenue data not available for ticker {ticker}\")\n",
    "            \n",
    "            revenue_data = financials[['Total Revenue']]\n",
    "            revenue_data = revenue_data.reset_index()\n",
    "            revenue_data.columns = ['Date', 'Revenue']\n",
    "            revenue_data['Date'] = pd.to_datetime(revenue_data['Date'])\n",
    "            revenue_data['Ticker'] = ticker\n",
    "            \n",
    "            logging.info(f\"Successfully fetched data for {ticker}\")\n",
    "            return revenue_data\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Attempt {attempt + 1} failed for {ticker}: {e}\")\n",
    "            time.sleep(5)  # Wait before retrying\n",
    "    logging.error(f\"Failed to fetch data for {ticker} after 3 attempts\")\n",
    "    return None\n",
    "\n",
    "# Load data for each ticker in parallel\n",
    "def load_all_data(tickers):\n",
    "    all_data = []\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:  # Adjust number of workers as needed\n",
    "        future_to_ticker = {executor.submit(load_financial_data, ticker): ticker for ticker in tickers}\n",
    "        for future in as_completed(future_to_ticker):\n",
    "            ticker = future_to_ticker[future]\n",
    "            try:\n",
    "                data = future.result()\n",
    "                if data is not None:\n",
    "                    all_data.append(data)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing ticker {ticker}: {e}\")\n",
    "    return all_data\n",
    "\n",
    "all_data = load_all_data(tickers)\n",
    "\n",
    "# Combine all data into one DataFrame\n",
    "df = pd.concat(all_data)\n",
    "\n",
    "# Standardize the start date\n",
    "common_start_date = df.groupby('Ticker')['Date'].min().max()\n",
    "df = df[df['Date'] >= common_start_date]\n",
    "\n",
    "# Forward fill missing values and backfill if necessary\n",
    "df = df.groupby('Ticker').apply(lambda group: group.set_index('Date').asfreq('M').ffill().bfill()).reset_index()\n",
    "\n",
    "# Prepare data function (modified to handle yfinance data)\n",
    "def prepare_data(df, ticker, cutoff_date):\n",
    "    df_ticker = df[df['Ticker'] == ticker]\n",
    "    df_ticker = df_ticker[['Date', 'Revenue']]\n",
    "    \n",
    "    # Split the data\n",
    "    train = df_ticker[df_ticker['Date'] < cutoff_date]\n",
    "    test = df_ticker[df_ticker['Date'] >= cutoff_date]\n",
    "    \n",
    "    if len(train) < 3:\n",
    "        raise ValueError(f\"Not enough data points for ticker {ticker} to create sequences.\")\n",
    "    \n",
    "    # Normalize the revenue\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_scaled = scaler.fit_transform(train[['Revenue']])\n",
    "    test_scaled = scaler.transform(test[['Revenue']])\n",
    "    \n",
    "    return train_scaled, test_scaled, scaler, train, test\n",
    "\n",
    "# Define the cutoff date\n",
    "cutoff_date = '2022-06-30'\n",
    "\n",
    "# Create models for each ticker\n",
    "models = {}\n",
    "results = []\n",
    "\n",
    "# Process data and train models\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        logging.info(f\"Processing ticker {ticker}\")\n",
    "        train_scaled, test_scaled, scaler, train, test = prepare_data(df, ticker, cutoff_date)\n",
    "        \n",
    "        # Determine sequence length based on available data\n",
    "        sequence_length = min(10, len(train_scaled) - 1)  # Increased sequence length to 10\n",
    "        \n",
    "        # Create sequences for LSTM\n",
    "        generator = TimeseriesGenerator(train_scaled, train_scaled, length=sequence_length, batch_size=16)\n",
    "        \n",
    "        # Build the model\n",
    "        model = Sequential([\n",
    "            LSTM(50, activation='relu', input_shape=(sequence_length, 1)),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(generator, epochs=5, verbose=1)  # Reduced epochs for quicker testing\n",
    "        \n",
    "        # Save the model\n",
    "        models[ticker] = model\n",
    "\n",
    "        # Predict the next 5 quarters\n",
    "        last_sequence = np.array([train_scaled[-sequence_length:]])\n",
    "        predictions_scaled = []\n",
    "        \n",
    "        for _ in range(5):\n",
    "            # Make prediction\n",
    "            prediction = model.predict(last_sequence)[0][0]\n",
    "            if not np.isfinite(prediction):  # Check if the prediction is a valid number\n",
    "                raise ValueError(f\"Invalid prediction for ticker {ticker}\")\n",
    "            predictions_scaled.append(prediction)\n",
    "            \n",
    "            # Update last_sequence with new prediction\n",
    "            new_sequence = np.array([[[prediction]]])\n",
    "            last_sequence = np.concatenate((last_sequence[:, 1:, :], new_sequence), axis=1)\n",
    "\n",
    "        # Inverse transform predictions\n",
    "        predictions = scaler.inverse_transform(np.array(predictions_scaled).reshape(-1, 1))\n",
    "        \n",
    "        # Evaluate the model\n",
    "        mae = mse = rmse = None\n",
    "        if len(test) > sequence_length:\n",
    "            test_sequences = TimeseriesGenerator(test_scaled, test_scaled, length=sequence_length, batch_size=1)\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            \n",
    "            for i in range(len(test_sequences)):\n",
    "                x, y = test_sequences[i]\n",
    "                y_true.append(y[0, 0])\n",
    "                y_pred.append(model.predict(x)[0, 0])\n",
    "            \n",
    "            y_true = np.array(y_true)\n",
    "            y_pred = np.array(y_pred)\n",
    "            \n",
    "            mae = mean_absolute_error(y_true, y_pred)\n",
    "            mse = mean_squared_error(y_true, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            \n",
    "            logging.info(f\"Metrics for {ticker} - MAE: {mae}, MSE: {mse}, RMSE: {rmse}\")\n",
    "        else:\n",
    "            logging.warning(f\"Not enough test data for evaluation for ticker {ticker}\")\n",
    "        \n",
    "        logging.info(f\"Predictions for {ticker}: {predictions.flatten()}\")\n",
    "\n",
    "        # Store results\n",
    "        for i, pred in enumerate(predictions.flatten(), start=1):\n",
    "            results.append([ticker, f'Q{i}', pred, mae, mse, rmse])\n",
    "\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"Error for ticker {ticker}: {e}\")\n",
    "        logging.info(f\"Skipping ticker {ticker} due to insufficient data.\")\n",
    "\n",
    "# Create DataFrame from results\n",
    "results_df = pd.DataFrame(results, columns=['Ticker', 'Quarter', 'Prediction', 'MAE', 'MSE', 'RMSE'])\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv('Revenue_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(         Date     Revenue      EBITDA      CapEx          NWC        UFCF  \\\n",
       " 0  2022-11-30  4816000000  1896000000 -121000000  14838000000  1141000000   \n",
       " 1  2023-02-28  4816000000  1896000000 -121000000  14838000000  1141000000   \n",
       " 2  2023-05-31  4816000000  1896000000 -121000000  14838000000  1141000000   \n",
       " 3  2023-08-31  4890000000  1988000000  -91000000  15776000000  1141000000   \n",
       " 4  2023-11-30  5048000000  2058000000  -47000000  16518000000  1363000000   \n",
       " \n",
       "   Ticker  \n",
       " 0   ADBE  \n",
       " 1   ADBE  \n",
       " 2   ADBE  \n",
       " 3   ADBE  \n",
       " 4   ADBE  ,\n",
       " Ticker\n",
       " ADBE    7\n",
       " VRNS    7\n",
       " CFLT    7\n",
       " PEGA    7\n",
       " CHKP    7\n",
       "        ..\n",
       " GDDY    6\n",
       " GEN     6\n",
       " ZUO     6\n",
       " IOT     5\n",
       " ADSK    5\n",
       " Name: count, Length: 173, dtype: int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'historical_data_final2.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to understand the structure and content\n",
    "df.head(), df['Ticker'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM model for EBITDA\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('historical_data_final.csv')\n",
    "\n",
    "# Convert the Date column to datetime and sort by date\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.sort_values(by='Date')\n",
    "\n",
    "# Extract unique tickers\n",
    "tickers = df['Ticker'].unique()\n",
    "\n",
    "# Prepare data\n",
    "def prepare_data(df, ticker, cutoff_date):\n",
    "    df_ticker = df[df['Ticker'] == ticker]\n",
    "    df_ticker = df_ticker[['Date', 'EBITDA']]\n",
    "    \n",
    "    # Split the data\n",
    "    train = df_ticker[df_ticker['Date'] < cutoff_date]\n",
    "    test = df_ticker[df_ticker['Date'] >= cutoff_date]\n",
    "    \n",
    "    if len(train) < 3:\n",
    "        raise ValueError(f\"Not enough data points for ticker {ticker} to create sequences.\")\n",
    "    \n",
    "    # Normalize the revenue\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_scaled = scaler.fit_transform(train[['EBITDA']])\n",
    "    test_scaled = scaler.transform(test[['EBITDA']])\n",
    "    \n",
    "    return train_scaled, test_scaled, scaler, train, test\n",
    "\n",
    "# Define the cutoff date\n",
    "cutoff_date = '2023-11-30'\n",
    "\n",
    "# Create models for each ticker\n",
    "models = {}\n",
    "results = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        train_scaled, test_scaled, scaler, train, test = prepare_data(df, ticker, cutoff_date)\n",
    "        \n",
    "        # Determine sequence length based on available data\n",
    "        sequence_length = min(2, len(train_scaled) - 1)\n",
    "        \n",
    "        # Create sequences for LSTM\n",
    "        generator = TimeseriesGenerator(train_scaled, train_scaled, length=sequence_length, batch_size=1)\n",
    "        \n",
    "        # Build the model\n",
    "        model = Sequential([\n",
    "            LSTM(50, activation='relu', input_shape=(sequence_length, 1)),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(generator, epochs=10, verbose=1)\n",
    "        \n",
    "        # Save the model\n",
    "        models[ticker] = model\n",
    "\n",
    "        # Predict the next 5 quarters\n",
    "        last_sequence = np.array([train_scaled[-sequence_length:]])\n",
    "        predictions_scaled = []\n",
    "        \n",
    "        for _ in range(5):\n",
    "            # Make prediction\n",
    "            prediction = model.predict(last_sequence)[0][0]\n",
    "            predictions_scaled.append(prediction)\n",
    "            \n",
    "            # Update last_sequence with new prediction\n",
    "            new_sequence = np.array([[[prediction]]])\n",
    "            last_sequence = np.concatenate((last_sequence[:, 1:, :], new_sequence), axis=1)\n",
    "\n",
    "        # Inverse transform predictions\n",
    "        predictions = scaler.inverse_transform(np.array(predictions_scaled).reshape(-1, 1))\n",
    "        \n",
    "        # Evaluate the model\n",
    "        mae = mse = rmse = None\n",
    "        if len(test) > sequence_length:\n",
    "            test_sequences = TimeseriesGenerator(test_scaled, test_scaled, length=sequence_length, batch_size=1)\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            \n",
    "            for i in range(len(test_sequences)):\n",
    "                x, y = test_sequences[i]\n",
    "                y_true.append(y[0, 0])\n",
    "                y_pred.append(model.predict(x)[0, 0])\n",
    "            \n",
    "            y_true = np.array(y_true)\n",
    "            y_pred = np.array(y_pred)\n",
    "            \n",
    "            mae = mean_absolute_error(y_true, y_pred)\n",
    "            mse = mean_squared_error(y_true, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            \n",
    "            print(f\"Metrics for {ticker}:\")\n",
    "            print(f\"  MAE: {mae}\")\n",
    "            print(f\"  MSE: {mse}\")\n",
    "            print(f\"  RMSE: {rmse}\")\n",
    "        else:\n",
    "            print(f\"Not enough test data for evaluation for ticker {ticker}\")\n",
    "        \n",
    "        print(f\"Predictions for {ticker}: {predictions.flatten()}\")\n",
    "\n",
    "        # Store results\n",
    "        for i, pred in enumerate(predictions.flatten(), start=1):\n",
    "            results.append([ticker, f'Q{i}', pred, mae, mse, rmse])\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        print(f\"Skipping ticker {ticker} due to insufficient data.\")\n",
    "\n",
    "# Create DataFrame from results\n",
    "results_df = pd.DataFrame(results, columns=['Ticker', 'Quarter', 'Prediction', 'MAE', 'MSE', 'RMSE'])\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv('EBITDApredictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM model for CapEx\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('historical_data_final.csv')\n",
    "\n",
    "# Convert the Date column to datetime and sort by date\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.sort_values(by='Date')\n",
    "\n",
    "# Extract unique tickers\n",
    "tickers = df['Ticker'].unique()\n",
    "\n",
    "# Prepare data\n",
    "def prepare_data(df, ticker, cutoff_date):\n",
    "    df_ticker = df[df['Ticker'] == ticker]\n",
    "    df_ticker = df_ticker[['Date', 'CapEx']]\n",
    "    \n",
    "    # Split the data\n",
    "    train = df_ticker[df_ticker['Date'] < cutoff_date]\n",
    "    test = df_ticker[df_ticker['Date'] >= cutoff_date]\n",
    "    \n",
    "    if len(train) < 3:\n",
    "        raise ValueError(f\"Not enough data points for ticker {ticker} to create sequences.\")\n",
    "    \n",
    "    # Normalize the revenue\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_scaled = scaler.fit_transform(train[['CapEx']])\n",
    "    test_scaled = scaler.transform(test[['CapEx']])\n",
    "    \n",
    "    return train_scaled, test_scaled, scaler, train, test\n",
    "\n",
    "# Define the cutoff date\n",
    "cutoff_date = '2023-11-30'\n",
    "\n",
    "# Create models for each ticker\n",
    "models = {}\n",
    "results = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        train_scaled, test_scaled, scaler, train, test = prepare_data(df, ticker, cutoff_date)\n",
    "        \n",
    "        # Determine sequence length based on available data\n",
    "        sequence_length = min(2, len(train_scaled) - 1)\n",
    "        \n",
    "        # Create sequences for LSTM\n",
    "        generator = TimeseriesGenerator(train_scaled, train_scaled, length=sequence_length, batch_size=1)\n",
    "        \n",
    "        # Build the model\n",
    "        model = Sequential([\n",
    "            LSTM(50, activation='relu', input_shape=(sequence_length, 1)),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(generator, epochs=10, verbose=1)\n",
    "        \n",
    "        # Save the model\n",
    "        models[ticker] = model\n",
    "\n",
    "        # Predict the next 5 quarters\n",
    "        last_sequence = np.array([train_scaled[-sequence_length:]])\n",
    "        predictions_scaled = []\n",
    "        \n",
    "        for _ in range(5):\n",
    "            # Make prediction\n",
    "            prediction = model.predict(last_sequence)[0][0]\n",
    "            predictions_scaled.append(prediction)\n",
    "            \n",
    "            # Update last_sequence with new prediction\n",
    "            new_sequence = np.array([[[prediction]]])\n",
    "            last_sequence = np.concatenate((last_sequence[:, 1:, :], new_sequence), axis=1)\n",
    "\n",
    "        # Inverse transform predictions\n",
    "        predictions = scaler.inverse_transform(np.array(predictions_scaled).reshape(-1, 1))\n",
    "        \n",
    "        # Evaluate the model\n",
    "        mae = mse = rmse = None\n",
    "        if len(test) > sequence_length:\n",
    "            test_sequences = TimeseriesGenerator(test_scaled, test_scaled, length=sequence_length, batch_size=1)\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            \n",
    "            for i in range(len(test_sequences)):\n",
    "                x, y = test_sequences[i]\n",
    "                y_true.append(y[0, 0])\n",
    "                y_pred.append(model.predict(x)[0, 0])\n",
    "            \n",
    "            y_true = np.array(y_true)\n",
    "            y_pred = np.array(y_pred)\n",
    "            \n",
    "            mae = mean_absolute_error(y_true, y_pred)\n",
    "            mse = mean_squared_error(y_true, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            \n",
    "            print(f\"Metrics for {ticker}:\")\n",
    "            print(f\"  MAE: {mae}\")\n",
    "            print(f\"  MSE: {mse}\")\n",
    "            print(f\"  RMSE: {rmse}\")\n",
    "        else:\n",
    "            print(f\"Not enough test data for evaluation for ticker {ticker}\")\n",
    "        \n",
    "        print(f\"Predictions for {ticker}: {predictions.flatten()}\")\n",
    "\n",
    "        # Store results\n",
    "        for i, pred in enumerate(predictions.flatten(), start=1):\n",
    "            results.append([ticker, f'Q{i}', pred, mae, mse, rmse])\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        print(f\"Skipping ticker {ticker} due to insufficient data.\")\n",
    "\n",
    "# Create DataFrame from results\n",
    "results_df = pd.DataFrame(results, columns=['Ticker', 'Quarter', 'Prediction', 'MAE', 'MSE', 'RMSE'])\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv('CapExpredictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM model for NWC\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('historical_data_final.csv')\n",
    "\n",
    "# Convert the Date column to datetime and sort by date\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.sort_values(by='Date')\n",
    "\n",
    "# Extract unique tickers\n",
    "tickers = df['Ticker'].unique()\n",
    "\n",
    "# Prepare data\n",
    "def prepare_data(df, ticker, cutoff_date):\n",
    "    df_ticker = df[df['Ticker'] == ticker]\n",
    "    df_ticker = df_ticker[['Date', 'NWC']]\n",
    "    \n",
    "    # Split the data\n",
    "    train = df_ticker[df_ticker['Date'] < cutoff_date]\n",
    "    test = df_ticker[df_ticker['Date'] >= cutoff_date]\n",
    "    \n",
    "    if len(train) < 3:\n",
    "        raise ValueError(f\"Not enough data points for ticker {ticker} to create sequences.\")\n",
    "    \n",
    "    # Normalize the revenue\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_scaled = scaler.fit_transform(train[['NWC']])\n",
    "    test_scaled = scaler.transform(test[['NWC']])\n",
    "    \n",
    "    return train_scaled, test_scaled, scaler, train, test\n",
    "\n",
    "# Define the cutoff date\n",
    "cutoff_date = '2023-11-30'\n",
    "\n",
    "# Create models for each ticker\n",
    "models = {}\n",
    "results = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        train_scaled, test_scaled, scaler, train, test = prepare_data(df, ticker, cutoff_date)\n",
    "        \n",
    "        # Determine sequence length based on available data\n",
    "        sequence_length = min(2, len(train_scaled) - 1)\n",
    "        \n",
    "        # Create sequences for LSTM\n",
    "        generator = TimeseriesGenerator(train_scaled, train_scaled, length=sequence_length, batch_size=1)\n",
    "        \n",
    "        # Build the model\n",
    "        model = Sequential([\n",
    "            LSTM(50, activation='relu', input_shape=(sequence_length, 1)),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(generator, epochs=10, verbose=1)\n",
    "        \n",
    "        # Save the model\n",
    "        models[ticker] = model\n",
    "\n",
    "        # Predict the next 5 quarters\n",
    "        last_sequence = np.array([train_scaled[-sequence_length:]])\n",
    "        predictions_scaled = []\n",
    "        \n",
    "        for _ in range(5):\n",
    "            # Make prediction\n",
    "            prediction = model.predict(last_sequence)[0][0]\n",
    "            predictions_scaled.append(prediction)\n",
    "            \n",
    "            # Update last_sequence with new prediction\n",
    "            new_sequence = np.array([[[prediction]]])\n",
    "            last_sequence = np.concatenate((last_sequence[:, 1:, :], new_sequence), axis=1)\n",
    "\n",
    "        # Inverse transform predictions\n",
    "        predictions = scaler.inverse_transform(np.array(predictions_scaled).reshape(-1, 1))\n",
    "        \n",
    "        # Evaluate the model\n",
    "        mae = mse = rmse = None\n",
    "        if len(test) > sequence_length:\n",
    "            test_sequences = TimeseriesGenerator(test_scaled, test_scaled, length=sequence_length, batch_size=1)\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            \n",
    "            for i in range(len(test_sequences)):\n",
    "                x, y = test_sequences[i]\n",
    "                y_true.append(y[0, 0])\n",
    "                y_pred.append(model.predict(x)[0, 0])\n",
    "            \n",
    "            y_true = np.array(y_true)\n",
    "            y_pred = np.array(y_pred)\n",
    "            \n",
    "            mae = mean_absolute_error(y_true, y_pred)\n",
    "            mse = mean_squared_error(y_true, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            \n",
    "            print(f\"Metrics for {ticker}:\")\n",
    "            print(f\"  MAE: {mae}\")\n",
    "            print(f\"  MSE: {mse}\")\n",
    "            print(f\"  RMSE: {rmse}\")\n",
    "        else:\n",
    "            print(f\"Not enough test data for evaluation for ticker {ticker}\")\n",
    "        \n",
    "        print(f\"Predictions for {ticker}: {predictions.flatten()}\")\n",
    "\n",
    "        # Store results\n",
    "        for i, pred in enumerate(predictions.flatten(), start=1):\n",
    "            results.append([ticker, f'Q{i}', pred, mae, mse, rmse])\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        print(f\"Skipping ticker {ticker} due to insufficient data.\")\n",
    "\n",
    "# Create DataFrame from results\n",
    "results_df = pd.DataFrame(results, columns=['Ticker', 'Quarter', 'Prediction', 'MAE', 'MSE', 'RMSE'])\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv('NWCpredictions.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
