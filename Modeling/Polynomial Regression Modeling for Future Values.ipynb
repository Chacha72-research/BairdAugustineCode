{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd981875",
   "metadata": {},
   "source": [
    "\n",
    "## Regression Models for Future Values\n",
    "This section includes code for training a polynomial regression machine learning model based on historical data to predict various factors. (as elaborated in our neural network/random forest training code)\n",
    "\n",
    "Polynomial regression is great when you’re dealing with data that doesn’t follow a straight-line pattern but has more of a curve to it. (our historic information!!!!!) It’s helpful for capturing non-linear relationships, like when the relationship between variables bends or curves rather than being linear. For instance, it’s useful in areas like modeling population growth, stock prices, or any situation where things grow at different rates over time. By adding powers of your features (like squared or cubic terms), it helps improve accuracy for complex patterns. But there’s a catch—using too high of a polynomial can lead to overfitting, meaning the model can get too detailed and fit random noise in your data rather than the actual trend. It’s flexible and often works well in practice, but you have to watch out for that overfitting risk. I decided to use a polynomial factor of 5, so that it would be of high accuracy, without overfitting to our data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for data manipulation, machine learning, and visualization\n",
    "import numpy as np  # For numerical operations\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "from sklearn.linear_model import LinearRegression  # For linear regression modeling\n",
    "from sklearn.preprocessing import RobustScaler, PolynomialFeatures  # For data scaling and feature engineering\n",
    "from sklearn.model_selection import TimeSeriesSplit  # For time series cross-validation\n",
    "from sklearn.pipeline import make_pipeline  # For creating a sequence of data transformations\n",
    "import matplotlib.pyplot as plt  # For creating visualizations\n",
    "\n",
    "# Define a function to prepare the data for analysis\n",
    "def prepare_data(df, tickers, min_data_points=5):\n",
    "    # Initialize an empty dictionary to store processed data\n",
    "    data_dict = {}\n",
    "    \n",
    "    # Iterate through each stock ticker\n",
    "    for ticker in tickers:\n",
    "        # Filter the dataframe for the current ticker and sort by year\n",
    "        df_ticker = df[df['Ticker'] == ticker]\n",
    "        df_ticker = df_ticker[['Year', 'Revenue']].sort_values('Year')\n",
    "\n",
    "        # Check if there's sufficient data for analysis\n",
    "        if len(df_ticker) < min_data_points:\n",
    "            print(f\"Skipping {ticker} due to insufficient data points ({len(df_ticker)}).\")\n",
    "            continue\n",
    "\n",
    "        # Scale the revenue data to reduce the impact of outliers\n",
    "        scaler = RobustScaler()\n",
    "        scaled_data = scaler.fit_transform(df_ticker[['Revenue']])\n",
    "\n",
    "        # Prepare input (X) and output (y) data\n",
    "        X = df_ticker['Year'].values.reshape(-1, 1)\n",
    "        y = scaled_data.reshape(-1)\n",
    "\n",
    "        # Check for any invalid data\n",
    "        if np.isnan(y).any():\n",
    "            print(f\"Skipping {ticker} due to NaN values in target variable y.\")\n",
    "            continue\n",
    "\n",
    "        # Store the processed data in the dictionary\n",
    "        data_dict[ticker] = {\n",
    "            'X': X,\n",
    "            'y': y,\n",
    "            'scaler': scaler,\n",
    "            'years': df_ticker['Year'].values\n",
    "        }\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "# Define a function to evaluate the model using cross-validation\n",
    "def evaluate_with_cross_validation(data_dict, tickers, degree=2, max_splits=5):\n",
    "    cv_results = []\n",
    "\n",
    "    # Iterate through each stock ticker\n",
    "    for ticker in tickers:\n",
    "        if ticker not in data_dict:  # Skip tickers that were filtered out\n",
    "            continue\n",
    "\n",
    "        X = data_dict[ticker]['X']\n",
    "        y = data_dict[ticker]['y']\n",
    "\n",
    "        # Determine the number of splits for cross-validation\n",
    "        n_splits = min(max_splits, len(X) - 1)\n",
    "        if n_splits < 2:\n",
    "            print(f\"Skipping cross-validation for {ticker} due to insufficient data points for splitting.\")\n",
    "            continue\n",
    "\n",
    "        # Perform time series cross-validation\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        ticker_cv_results = []\n",
    "\n",
    "        for train_index, test_index in tscv.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            # Create and train the model\n",
    "            model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Evaluate the model's performance\n",
    "            test_loss = np.mean((model.predict(X_test) - y_test) ** 2)\n",
    "            print(f\"Test Loss for {ticker} fold: {test_loss}\")\n",
    "            ticker_cv_results.append(test_loss)\n",
    "\n",
    "        # Calculate and store the average cross-validation loss\n",
    "        avg_cv_loss = np.mean(ticker_cv_results)\n",
    "        cv_results.append({\n",
    "            'Ticker': ticker,\n",
    "            'CV_Loss': avg_cv_loss\n",
    "        })\n",
    "\n",
    "    return cv_results\n",
    "\n",
    "# Load the financial data from a CSV file\n",
    "df = pd.read_csv('historical_financial_data_eodhdsecondversion.csv') #our previous historic data! Now, we want to make predictions off of that in this model!\n",
    "\n",
    "# Get the list of unique stock tickers\n",
    "tickers = df['Ticker'].unique()\n",
    "\n",
    "# Prepare the data for analysis\n",
    "data_dict = prepare_data(df, tickers, min_data_points=5)\n",
    "\n",
    "# Set the complexity of the polynomial model\n",
    "poly_degree = 5\n",
    "\n",
    "# Evaluate the model using cross-validation\n",
    "cv_results = evaluate_with_cross_validation(data_dict, tickers, degree=poly_degree)\n",
    "\n",
    "# Initialize an empty list to store prediction results\n",
    "results = []\n",
    "\n",
    "# Iterate through each stock ticker\n",
    "for ticker in tickers:\n",
    "    if ticker not in data_dict:  # Skip tickers that were filtered out\n",
    "        continue\n",
    "    \n",
    "    data = data_dict[ticker]\n",
    "    X, y, scaler = data['X'], data['y'], data['scaler']\n",
    "\n",
    "    # Create and train the model\n",
    "    model = make_pipeline(PolynomialFeatures(poly_degree), LinearRegression())\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Generate predictions for the next 3 years\n",
    "    last_year = X[-1, 0]\n",
    "    future_years = np.array(range(last_year + 1, last_year + 4)).reshape(-1, 1)\n",
    "    predictions = model.predict(future_years)\n",
    "\n",
    "    # Convert predictions back to original scale\n",
    "    predictions_scaled = scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "    print(f\"Predictions for {ticker}:\", predictions_scaled)\n",
    "\n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'Ticker': ticker,\n",
    "        'Predictions': predictions_scaled.tolist()\n",
    "    })\n",
    "\n",
    "    # Visualize the actual vs predicted values\n",
    "    actual_values = scaler.inverse_transform(y.reshape(-1, 1)).flatten()\n",
    "    years = data['years']\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(years, actual_values, label='Actual Values', color='blue')\n",
    "    plt.plot(years, scaler.inverse_transform(model.predict(X).reshape(-1, 1)).flatten(), \n",
    "             label='Fitted Curve', color='red')\n",
    "    plt.plot(future_years, predictions_scaled, label='Predictions', linestyle='--', color='green')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Revenue')\n",
    "    plt.title(f'Actual vs Predicted Revenue for {ticker} (Polynomial Degree: {poly_degree})')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Convert results to DataFrames for easy handling\n",
    "results_df = pd.DataFrame(results)\n",
    "results_cv_df = pd.DataFrame(cv_results)\n",
    "\n",
    "# Save the results to CSV files\n",
    "results_df.to_csv('revenue_model_predictions_polynomial_regression3.csv', index=False)\n",
    "\n",
    "print(\"Results saved to revenue_model_predictions_polynomial_regression2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions for EBITDA - same as above\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import RobustScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def prepare_data(df, tickers, min_data_points=5):\n",
    "    data_dict = {}\n",
    "    for ticker in tickers:\n",
    "        df_ticker = df[df['Ticker'] == ticker]\n",
    "        df_ticker = df_ticker[['Year', 'EBITDA']].sort_values('Year')\n",
    "\n",
    "        # Check if the ticker has at least 'min_data_points' data points\n",
    "        if len(df_ticker) < min_data_points:\n",
    "            print(f\"Skipping {ticker} due to insufficient data points ({len(df_ticker)}).\")\n",
    "            continue\n",
    "\n",
    "        scaler = RobustScaler()\n",
    "        scaled_data = scaler.fit_transform(df_ticker[['EBITDA']])\n",
    "\n",
    "        X = df_ticker['Year'].values.reshape(-1, 1)\n",
    "        y = scaled_data.reshape(-1)\n",
    "\n",
    "        # Check for NaN values in y and skip the ticker if found\n",
    "        if np.isnan(y).any():\n",
    "            print(f\"Skipping {ticker} due to NaN values in target variable y.\")\n",
    "            continue\n",
    "\n",
    "        data_dict[ticker] = {\n",
    "            'X': X,\n",
    "            'y': y,\n",
    "            'scaler': scaler,\n",
    "            'years': df_ticker['Year'].values\n",
    "        }\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "def evaluate_with_cross_validation(data_dict, tickers, degree=2, max_splits=5):\n",
    "    cv_results = []\n",
    "\n",
    "    for ticker in tickers:\n",
    "        if ticker not in data_dict:  # Skip tickers that were filtered out\n",
    "            continue\n",
    "\n",
    "        X = data_dict[ticker]['X']\n",
    "        y = data_dict[ticker]['y']\n",
    "\n",
    "        # Adjust n_splits to be less than the number of data points\n",
    "        n_splits = min(max_splits, len(X) - 1)\n",
    "        if n_splits < 2:\n",
    "            print(f\"Skipping cross-validation for {ticker} due to insufficient data points for splitting.\")\n",
    "            continue\n",
    "\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        ticker_cv_results = []\n",
    "\n",
    "        for train_index, test_index in tscv.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            test_loss = np.mean((model.predict(X_test) - y_test) ** 2)\n",
    "            print(f\"Test Loss for {ticker} fold: {test_loss}\")\n",
    "            ticker_cv_results.append(test_loss)\n",
    "\n",
    "        avg_cv_loss = np.mean(ticker_cv_results)\n",
    "        cv_results.append({\n",
    "            'Ticker': ticker,\n",
    "            'CV_Loss': avg_cv_loss\n",
    "        })\n",
    "\n",
    "    return cv_results\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('historical_financial_data_eodhdsecondversion.csv')\n",
    "\n",
    "# List of tickers to include\n",
    "tickers = df['Ticker'].unique()\n",
    "\n",
    "# Prepare data with a minimum data points threshold\n",
    "data_dict = prepare_data(df, tickers, min_data_points=5)\n",
    "\n",
    "# Set the degree of the polynomial \n",
    "poly_degree = 5\n",
    "\n",
    "# Evaluate with cross-validation\n",
    "cv_results = evaluate_with_cross_validation(data_dict, tickers, degree=poly_degree)\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "results = []\n",
    "\n",
    "# Iterate over each ticker\n",
    "for ticker in tickers:\n",
    "    if ticker not in data_dict:  # Skip tickers that were filtered out\n",
    "        continue\n",
    "    \n",
    "    data = data_dict[ticker]\n",
    "    X, y, scaler = data['X'], data['y'], data['scaler']\n",
    "\n",
    "    # Build and fit the model\n",
    "    model = make_pipeline(PolynomialFeatures(poly_degree), LinearRegression())\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Make predictions for the next 5 years\n",
    "    last_year = X[-1, 0]\n",
    "    future_years = np.array(range(last_year + 1, last_year + 4)).reshape(-1, 1)\n",
    "    predictions = model.predict(future_years)\n",
    "\n",
    "    # Inverse transform predictions\n",
    "    predictions_scaled = scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "    print(f\"Predictions for {ticker}:\", predictions_scaled)\n",
    "\n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'Ticker': ticker,\n",
    "        'Predictions': predictions_scaled.tolist()\n",
    "    })\n",
    "\n",
    "    # Plotting the actual vs predicted values\n",
    "    actual_values = scaler.inverse_transform(y.reshape(-1, 1)).flatten()\n",
    "    years = data['years']\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(years, actual_values, label='Actual Values', color='blue')\n",
    "    plt.plot(years, scaler.inverse_transform(model.predict(X).reshape(-1, 1)).flatten(), \n",
    "             label='Fitted Curve', color='red')\n",
    "    plt.plot(future_years, predictions_scaled, label='Predictions', linestyle='--', color='green')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Revenue')\n",
    "    plt.title(f'Actual vs Predicted Revenue for {ticker} (Polynomial Degree: {poly_degree})')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Convert results to DataFrames\n",
    "results_df = pd.DataFrame(results)\n",
    "results_cv_df = pd.DataFrame(cv_results)\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv('ebitda_model_predictions.csv', index=False)\n",
    "\n",
    "import ast\n",
    "df = pd.read_csv('ebitda_model_predictions.csv')\n",
    "\n",
    "# Convert the Predictions column from string representation of lists to actual lists\n",
    "df['Predictions'] = df['Predictions'].apply(ast.literal_eval)\n",
    "\n",
    "# Create a list to store the new rows for the long-format DataFrame\n",
    "long_format_data = []\n",
    "\n",
    "# Loop through each row in the original DataFrame\n",
    "for _, row in df.iterrows():\n",
    "    ticker = row['Ticker']  # Assuming there is a 'Ticker' column\n",
    "    predictions = row['Predictions']\n",
    "    \n",
    "    # Loop through each prediction and create a new row for each year\n",
    "    for year_idx, revenue in enumerate(predictions):\n",
    "        year = 2023 + year_idx + 1  # Adjust the year as needed\n",
    "        long_format_data.append([year, ticker, revenue])\n",
    "\n",
    "# Create a new DataFrame with the long-format data\n",
    "long_format_df = pd.DataFrame(long_format_data, columns=['Year', 'Ticker', 'Revenue'])\n",
    "\n",
    "# Save the long-format DataFrame to a new CSV file\n",
    "long_format_df.to_csv('ebitda_model_predictions_formatted.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction for market cap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import RobustScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def prepare_data(df, tickers, min_data_points=5):\n",
    "    data_dict = {}\n",
    "    for ticker in tickers:\n",
    "        df_ticker = df[df['Ticker'] == ticker]\n",
    "        df_ticker = df_ticker[['Year', 'Market Capitalization']].sort_values('Year')\n",
    "\n",
    "        # Check if the ticker has at least 'min_data_points' data points\n",
    "        if len(df_ticker) < min_data_points:\n",
    "            print(f\"Skipping {ticker} due to insufficient data points ({len(df_ticker)}).\")\n",
    "            continue\n",
    "\n",
    "        scaler = RobustScaler()\n",
    "        scaled_data = scaler.fit_transform(df_ticker[['Market Capitalization']])\n",
    "\n",
    "        X = df_ticker['Year'].values.reshape(-1, 1)\n",
    "        y = scaled_data.reshape(-1)\n",
    "\n",
    "        # Check for NaN values in y and skip the ticker if found\n",
    "        if np.isnan(y).any():\n",
    "            print(f\"Skipping {ticker} due to NaN values in target variable y.\")\n",
    "            continue\n",
    "\n",
    "        data_dict[ticker] = {\n",
    "            'X': X,\n",
    "            'y': y,\n",
    "            'scaler': scaler,\n",
    "            'years': df_ticker['Year'].values\n",
    "        }\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "def evaluate_with_cross_validation(data_dict, tickers, degree=2, max_splits=5):\n",
    "    cv_results = []\n",
    "\n",
    "    for ticker in tickers:\n",
    "        if ticker not in data_dict:  # Skip tickers that were filtered out\n",
    "            continue\n",
    "\n",
    "        X = data_dict[ticker]['X']\n",
    "        y = data_dict[ticker]['y']\n",
    "\n",
    "        # Adjust n_splits to be less than the number of data points\n",
    "        n_splits = min(max_splits, len(X) - 1)\n",
    "        if n_splits < 2:\n",
    "            print(f\"Skipping cross-validation for {ticker} due to insufficient data points for splitting.\")\n",
    "            continue\n",
    "\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        ticker_cv_results = []\n",
    "\n",
    "        for train_index, test_index in tscv.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            test_loss = np.mean((model.predict(X_test) - y_test) ** 2)\n",
    "            print(f\"Test Loss for {ticker} fold: {test_loss}\")\n",
    "            ticker_cv_results.append(test_loss)\n",
    "\n",
    "        avg_cv_loss = np.mean(ticker_cv_results)\n",
    "        cv_results.append({\n",
    "            'Ticker': ticker,\n",
    "            'CV_Loss': avg_cv_loss\n",
    "        })\n",
    "\n",
    "    return cv_results\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('historical_financial_data_eodhdsecondversion.csv')\n",
    "\n",
    "# List of tickers to include\n",
    "tickers = df['Ticker'].unique()\n",
    "\n",
    "# Prepare data with a minimum data points threshold\n",
    "data_dict = prepare_data(df, tickers, min_data_points=5)\n",
    "\n",
    "# Set the degree of the polynomial \n",
    "poly_degree = 5\n",
    "\n",
    "# Evaluate with cross-validation\n",
    "cv_results = evaluate_with_cross_validation(data_dict, tickers, degree=poly_degree)\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "results = []\n",
    "\n",
    "# Iterate over each ticker\n",
    "for ticker in tickers:\n",
    "    if ticker not in data_dict:  # Skip tickers that were filtered out\n",
    "        continue\n",
    "    \n",
    "    data = data_dict[ticker]\n",
    "    X, y, scaler = data['X'], data['y'], data['scaler']\n",
    "\n",
    "    # Build and fit the model\n",
    "    model = make_pipeline(PolynomialFeatures(poly_degree), LinearRegression())\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Make predictions for the next 5 years\n",
    "    last_year = X[-1, 0]\n",
    "    future_years = np.array(range(last_year + 1, last_year + 4)).reshape(-1, 1)\n",
    "    predictions = model.predict(future_years)\n",
    "\n",
    "    # Inverse transform predictions\n",
    "    predictions_scaled = scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "    print(f\"Predictions for {ticker}:\", predictions_scaled)\n",
    "\n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'Ticker': ticker,\n",
    "        'Predictions': predictions_scaled.tolist()\n",
    "    })\n",
    "\n",
    "    # Plotting the actual vs predicted values\n",
    "    actual_values = scaler.inverse_transform(y.reshape(-1, 1)).flatten()\n",
    "    years = data['years']\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(years, actual_values, label='Actual Values', color='blue')\n",
    "    plt.plot(years, scaler.inverse_transform(model.predict(X).reshape(-1, 1)).flatten(), \n",
    "             label='Fitted Curve', color='red')\n",
    "    plt.plot(future_years, predictions_scaled, label='Predictions', linestyle='--', color='green')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Revenue')\n",
    "    plt.title(f'Actual vs Predicted Revenue for {ticker} (Polynomial Degree: {poly_degree})')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Convert results to DataFrames\n",
    "results_df = pd.DataFrame(results)\n",
    "results_cv_df = pd.DataFrame(cv_results)\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv('mc_model_predictions.csv', index=False)\n",
    "\n",
    "import ast\n",
    "df = pd.read_csv('mc_model_predictions.csv')\n",
    "\n",
    "# Convert the Predictions column from string representation of lists to actual lists\n",
    "df['Predictions'] = df['Predictions'].apply(ast.literal_eval)\n",
    "\n",
    "# Create a list to store the new rows for the long-format DataFrame\n",
    "long_format_data = []\n",
    "\n",
    "# Loop through each row in the original DataFrame\n",
    "for _, row in df.iterrows():\n",
    "    ticker = row['Ticker']  # Assuming there is a 'Ticker' column\n",
    "    predictions = row['Predictions']\n",
    "    \n",
    "    # Loop through each prediction and create a new row for each year\n",
    "    for year_idx, revenue in enumerate(predictions):\n",
    "        year = 2023 + year_idx + 1  # Adjust the year as needed\n",
    "        long_format_data.append([year, ticker, revenue])\n",
    "\n",
    "# Create a new DataFrame with the long-format data\n",
    "long_format_df = pd.DataFrame(long_format_data, columns=['Year', 'Ticker', 'Market Capitalization'])\n",
    "\n",
    "# Save the long-format DataFrame to a new CSV file\n",
    "long_format_df.to_csv('mc_model_predictions_formatted.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction for Gross Profit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import RobustScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def prepare_data(df, tickers, min_data_points=5):\n",
    "    data_dict = {}\n",
    "    for ticker in tickers:\n",
    "        df_ticker = df[df['Ticker'] == ticker]\n",
    "        df_ticker = df_ticker[['Year', 'Gross Profit']].sort_values('Year')\n",
    "\n",
    "        # Check if the ticker has at least 'min_data_points' data points\n",
    "        if len(df_ticker) < min_data_points:\n",
    "            print(f\"Skipping {ticker} due to insufficient data points ({len(df_ticker)}).\")\n",
    "            continue\n",
    "\n",
    "        scaler = RobustScaler()\n",
    "        scaled_data = scaler.fit_transform(df_ticker[['Gross Profit']])\n",
    "\n",
    "        X = df_ticker['Year'].values.reshape(-1, 1)\n",
    "        y = scaled_data.reshape(-1)\n",
    "\n",
    "        # Check for NaN values in y and skip the ticker if found\n",
    "        if np.isnan(y).any():\n",
    "            print(f\"Skipping {ticker} due to NaN values in target variable y.\")\n",
    "            continue\n",
    "\n",
    "        data_dict[ticker] = {\n",
    "            'X': X,\n",
    "            'y': y,\n",
    "            'scaler': scaler,\n",
    "            'years': df_ticker['Year'].values\n",
    "        }\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "def evaluate_with_cross_validation(data_dict, tickers, degree=2, max_splits=5):\n",
    "    cv_results = []\n",
    "\n",
    "    for ticker in tickers:\n",
    "        if ticker not in data_dict:  # Skip tickers that were filtered out\n",
    "            continue\n",
    "\n",
    "        X = data_dict[ticker]['X']\n",
    "        y = data_dict[ticker]['y']\n",
    "\n",
    "        # Adjust n_splits to be less than the number of data points\n",
    "        n_splits = min(max_splits, len(X) - 1)\n",
    "        if n_splits < 2:\n",
    "            print(f\"Skipping cross-validation for {ticker} due to insufficient data points for splitting.\")\n",
    "            continue\n",
    "\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        ticker_cv_results = []\n",
    "\n",
    "        for train_index, test_index in tscv.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            test_loss = np.mean((model.predict(X_test) - y_test) ** 2)\n",
    "            print(f\"Test Loss for {ticker} fold: {test_loss}\")\n",
    "            ticker_cv_results.append(test_loss)\n",
    "\n",
    "        avg_cv_loss = np.mean(ticker_cv_results)\n",
    "        cv_results.append({\n",
    "            'Ticker': ticker,\n",
    "            'CV_Loss': avg_cv_loss\n",
    "        })\n",
    "\n",
    "    return cv_results\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('historical_financial_data_eodhdsecondversion.csv')\n",
    "\n",
    "# List of tickers to include\n",
    "tickers = df['Ticker'].unique()\n",
    "\n",
    "# Prepare data with a minimum data points threshold\n",
    "data_dict = prepare_data(df, tickers, min_data_points=5)\n",
    "\n",
    "# Set the degree of the polynomial \n",
    "poly_degree = 5\n",
    "\n",
    "# Evaluate with cross-validation\n",
    "cv_results = evaluate_with_cross_validation(data_dict, tickers, degree=poly_degree)\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "results = []\n",
    "\n",
    "# Iterate over each ticker\n",
    "for ticker in tickers:\n",
    "    if ticker not in data_dict:  # Skip tickers that were filtered out\n",
    "        continue\n",
    "    \n",
    "    data = data_dict[ticker]\n",
    "    X, y, scaler = data['X'], data['y'], data['scaler']\n",
    "\n",
    "    # Build and fit the model\n",
    "    model = make_pipeline(PolynomialFeatures(poly_degree), LinearRegression())\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Make predictions for the next 5 years\n",
    "    last_year = X[-1, 0]\n",
    "    future_years = np.array(range(last_year + 1, last_year + 4)).reshape(-1, 1)\n",
    "    predictions = model.predict(future_years)\n",
    "\n",
    "    # Inverse transform predictions\n",
    "    predictions_scaled = scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "    print(f\"Predictions for {ticker}:\", predictions_scaled)\n",
    "\n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'Ticker': ticker,\n",
    "        'Predictions': predictions_scaled.tolist()\n",
    "    })\n",
    "\n",
    "    # Plotting the actual vs predicted values\n",
    "    actual_values = scaler.inverse_transform(y.reshape(-1, 1)).flatten()\n",
    "    years = data['years']\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(years, actual_values, label='Actual Values', color='blue')\n",
    "    plt.plot(years, scaler.inverse_transform(model.predict(X).reshape(-1, 1)).flatten(), \n",
    "             label='Fitted Curve', color='red')\n",
    "    plt.plot(future_years, predictions_scaled, label='Predictions', linestyle='--', color='green')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Revenue')\n",
    "    plt.title(f'Actual vs Predicted gp for {ticker} (Polynomial Degree: {poly_degree})')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Convert results to DataFrames\n",
    "results_df = pd.DataFrame(results)\n",
    "results_cv_df = pd.DataFrame(cv_results)\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv('gp_model_predictions.csv', index=False)\n",
    "\n",
    "import ast\n",
    "df = pd.read_csv('gp_model_predictions.csv')\n",
    "\n",
    "# Convert the Predictions column from string representation of lists to actual lists\n",
    "df['Predictions'] = df['Predictions'].apply(ast.literal_eval)\n",
    "\n",
    "# Create a list to store the new rows for the long-format DataFrame\n",
    "long_format_data = []\n",
    "\n",
    "# Loop through each row in the original DataFrame\n",
    "for _, row in df.iterrows():\n",
    "    ticker = row['Ticker']  # Assuming there is a 'Ticker' column\n",
    "    predictions = row['Predictions']\n",
    "    \n",
    "    # Loop through each prediction and create a new row for each year\n",
    "    for year_idx, revenue in enumerate(predictions):\n",
    "        year = 2023 + year_idx + 1  # Adjust the year as needed\n",
    "        long_format_data.append([year, ticker, revenue])\n",
    "\n",
    "# Create a new DataFrame with the long-format data\n",
    "long_format_df = pd.DataFrame(long_format_data, columns=['Year', 'Ticker', 'Gross Profit'])\n",
    "\n",
    "# Save the long-format DataFrame to a new CSV file\n",
    "long_format_df.to_csv('gp_model_predictions_formatted.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction for Total Assets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import RobustScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def prepare_data(df, tickers, min_data_points=5):\n",
    "    data_dict = {}\n",
    "    for ticker in tickers:\n",
    "        df_ticker = df[df['Ticker'] == ticker]\n",
    "        df_ticker = df_ticker[['Year', 'Total Assets']].sort_values('Year')\n",
    "\n",
    "        # Check if the ticker has at least 'min_data_points' data points\n",
    "        if len(df_ticker) < min_data_points:\n",
    "            print(f\"Skipping {ticker} due to insufficient data points ({len(df_ticker)}).\")\n",
    "            continue\n",
    "\n",
    "        scaler = RobustScaler()\n",
    "        scaled_data = scaler.fit_transform(df_ticker[['Total Assets']])\n",
    "\n",
    "        X = df_ticker['Year'].values.reshape(-1, 1)\n",
    "        y = scaled_data.reshape(-1)\n",
    "\n",
    "        # Check for NaN values in y and skip the ticker if found\n",
    "        if np.isnan(y).any():\n",
    "            print(f\"Skipping {ticker} due to NaN values in target variable y.\")\n",
    "            continue\n",
    "\n",
    "        data_dict[ticker] = {\n",
    "            'X': X,\n",
    "            'y': y,\n",
    "            'scaler': scaler,\n",
    "            'years': df_ticker['Year'].values\n",
    "        }\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "def evaluate_with_cross_validation(data_dict, tickers, degree=2, max_splits=5):\n",
    "    cv_results = []\n",
    "\n",
    "    for ticker in tickers:\n",
    "        if ticker not in data_dict:  # Skip tickers that were filtered out\n",
    "            continue\n",
    "\n",
    "        X = data_dict[ticker]['X']\n",
    "        y = data_dict[ticker]['y']\n",
    "\n",
    "        # Adjust n_splits to be less than the number of data points\n",
    "        n_splits = min(max_splits, len(X) - 1)\n",
    "        if n_splits < 2:\n",
    "            print(f\"Skipping cross-validation for {ticker} due to insufficient data points for splitting.\")\n",
    "            continue\n",
    "\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        ticker_cv_results = []\n",
    "\n",
    "        for train_index, test_index in tscv.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            test_loss = np.mean((model.predict(X_test) - y_test) ** 2)\n",
    "            print(f\"Test Loss for {ticker} fold: {test_loss}\")\n",
    "            ticker_cv_results.append(test_loss)\n",
    "\n",
    "        avg_cv_loss = np.mean(ticker_cv_results)\n",
    "        cv_results.append({\n",
    "            'Ticker': ticker,\n",
    "            'CV_Loss': avg_cv_loss\n",
    "        })\n",
    "\n",
    "    return cv_results\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('historical_financial_data_eodhdsecondversion.csv')\n",
    "\n",
    "# List of tickers to include\n",
    "tickers = df['Ticker'].unique()\n",
    "\n",
    "# Prepare data with a minimum data points threshold\n",
    "data_dict = prepare_data(df, tickers, min_data_points=5)\n",
    "\n",
    "# Set the degree of the polynomial \n",
    "poly_degree = 5\n",
    "\n",
    "# Evaluate with cross-validation\n",
    "cv_results = evaluate_with_cross_validation(data_dict, tickers, degree=poly_degree)\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "results = []\n",
    "\n",
    "# Iterate over each ticker\n",
    "for ticker in tickers:\n",
    "    if ticker not in data_dict:  # Skip tickers that were filtered out\n",
    "        continue\n",
    "    \n",
    "    data = data_dict[ticker]\n",
    "    X, y, scaler = data['X'], data['y'], data['scaler']\n",
    "\n",
    "    # Build and fit the model\n",
    "    model = make_pipeline(PolynomialFeatures(poly_degree), LinearRegression())\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Make predictions for the next 5 years\n",
    "    last_year = X[-1, 0]\n",
    "    future_years = np.array(range(last_year + 1, last_year + 4)).reshape(-1, 1)\n",
    "    predictions = model.predict(future_years)\n",
    "\n",
    "    # Inverse transform predictions\n",
    "    predictions_scaled = scaler.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "    print(f\"Predictions for {ticker}:\", predictions_scaled)\n",
    "\n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'Ticker': ticker,\n",
    "        'Predictions': predictions_scaled.tolist()\n",
    "    })\n",
    "\n",
    "    # Plotting the actual vs predicted values\n",
    "    actual_values = scaler.inverse_transform(y.reshape(-1, 1)).flatten()\n",
    "    years = data['years']\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(years, actual_values, label='Actual Values', color='blue')\n",
    "    plt.plot(years, scaler.inverse_transform(model.predict(X).reshape(-1, 1)).flatten(), \n",
    "             label='Fitted Curve', color='red')\n",
    "    plt.plot(future_years, predictions_scaled, label='Predictions', linestyle='--', color='green')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Revenue')\n",
    "    plt.title(f'Actual vs Predicted nwc for {ticker} (Polynomial Degree: {poly_degree})')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Convert results to DataFrames\n",
    "results_df = pd.DataFrame(results)\n",
    "results_cv_df = pd.DataFrame(cv_results)\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv('ta_model_predictions.csv', index=False)\n",
    "\n",
    "import ast\n",
    "df = pd.read_csv('ta_model_predictions.csv')\n",
    "\n",
    "# Convert the Predictions column from string representation of lists to actual lists\n",
    "df['Predictions'] = df['Predictions'].apply(ast.literal_eval)\n",
    "\n",
    "# Create a list to store the new rows for the long-format DataFrame\n",
    "long_format_data = []\n",
    "\n",
    "# Loop through each row in the original DataFrame\n",
    "for _, row in df.iterrows():\n",
    "    ticker = row['Ticker']  # Assuming there is a 'Ticker' column\n",
    "    predictions = row['Predictions']\n",
    "    \n",
    "    # Loop through each prediction and create a new row for each year\n",
    "    for year_idx, revenue in enumerate(predictions):\n",
    "        year = 2023 + year_idx + 1  # Adjust the year as needed\n",
    "        long_format_data.append([year, ticker, revenue])\n",
    "\n",
    "# Create a new DataFrame with the long-format data\n",
    "long_format_df = pd.DataFrame(long_format_data, columns=['Year', 'Ticker', 'Total Assets'])\n",
    "\n",
    "# Save the long-format DataFrame to a new CSV file\n",
    "long_format_df.to_csv('ta_model_predictions_formatted.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the saved csv files were merged and compiled here: https://docs.google.com/spreadsheets/d/124PCo1J2RGgyKaPO4naDsoqotDKHtOZD9wzmjizC5_Y/edit?usp=sharing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
